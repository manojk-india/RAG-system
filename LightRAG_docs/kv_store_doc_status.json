{
  "doc-097dc8ba56ac4aaaea4563fdb491cc8a": {
    "status": "pending",
    "content": "Mutation-Guided LLM-based Test Generation at Meta\nChristopherFoster AbhishekGulati MarkHarman\nProductComplianceandPrivacy ProductComplianceandPrivacy ProductComplianceandPrivacy\nteam,MetaPlatforms team,MetaPlatforms team,MetaPlatforms\nMenloPark,USA MenloPark,USA London,UK\nInnaHarper KeMao JillianRitchey\nDeveloperInfrastructureteam,Meta WhatsAppteam,MetaPlatforms Messengerteam,MetaPlatforms\nPlatforms London,UK NewYork,USA\nLondon,UK\nHervéRobert ShubhoSengupta\nProductComplianceandPrivacy FAIR,MetaPlatforms\nteam,MetaPlatforms MenloPark,USA\nMenloPark,USA\nABSTRACT ACMReferenceFormat:\nThis paper1 describes Meta’s ACH system for mutation-guided ChristopherFoster,AbhishekGulati,MarkHarman,InnaHarper,KeMao,\nJillianRitchey,HervéRobert,andShubhoSengupta.2024.Mutation-Guided\nLLM-basedtestgeneration.ACHgeneratesrelativelyfewmutants\nLLM-basedTestGenerationatMeta.InCompanionProceedingsofthe33rd\n(akasimulatedfaults),comparedtotraditionalmutationtesting.In-\nACMInternationalConferenceontheFoundationsofSoftwareEngineering\nstead,itfocusesongeneratingcurrentlyundetectedfaultsthatare\n(FSECompanion’25),23–27,2025,Trondheim,Norway.ACM,NewYork,\nspecifictoanissueofconcern.Fromthesecurrentlyuncaughtfaults,\nNY,USA,12pages.https://doi.org/10.1145/3663529.3663839\nACHgeneratesteststhatcancatchthem,thereby‘killing’themu-\ntantsandconsequentlyhardeningtheplatformagainstregressions.\nWeuseprivacyconcernstoillustrateourapproach,butACHcan 1 INTRODUCTION\nharden code against any type of regression. In total, ACH was Inthispaper,wereportonMeta’sdeploymentofACH2,anagen-\nappliedto10,795AndroidKotlinclassesin7softwareplatforms ticLLM-basedtoolforgeneratingteststotargetspecificclasses\ndeployed by Meta, from which it generated 9,095 mutants and offaults.Thepaperfocusesonautomatedprivacyhardening:the\n571privacy-hardeningtestcases.ACHalsodeploysanLLM-based problemofautomaticallygeneratingunitteststoreducetherisk\nequivalent mutant detection agent that achieves a precision of offutureregressionswithrespecttoprivacyissues.However,the\n0.79andarecallof0.47(risingto0.95and0.96withsimplepre- approachcanbeappliedtoanyissueandisnotconfinedsolely\nprocessing).ACHwasusedbyMessengerandWhatsApptest-a- totacklingprivacyissues.Thepaperreportsresultsfromthede-\nthonswhereengineersaccepted73%ofitstests,judging36%to ploymentofACHatMetabetween28thOctober2024and31st\nprivacyrelevant.WeconcludethatACHhardenscodeagainstspe- December2024.\ncificconcernsandthat,evenwhenitstestsdonotdirectlytacklethe AlthoughtherehasbeenagreatdealofrecentattentiononLLM-\nspecificconcern,engineersfindthemusefulfortheirotherbenefits. basedtestgeneration[3,18,39,52,53],therehasbeenlittlework\nondevelopingtestsforspecificclassesoffault.Manycompanies\nCCSCONCEPTS\nhaveexposuretospecifichighimpactfaultsrelatedtoimportant\n•Softwareanditsengineering→Softwaretestinganddebug- issuessuchassecurity,integrity,andprivacy.Theimportanceof\nging. such issues makes it equally important to have test generation\ntechniquesthatcantargetthesespecificclassesoffaults.\nKEYWORDS Organizationstypicallycollectdataaboutbugsfoundduringde-\nvelopment.Thisprovidesarichsourceofinformationwithwhich\nUnitTesting,AutomatedTestGeneration,LargeLanguageModels,\ntoguidetestgeneration.Thechallenge,therefore,istofindaway\nLLMs.\ntogenerateteststhattargetspecificissuesonthebasisofthisinfor-\n1Authororderisalphabetical.ThecorrespondingauthorisMarkHarman. mation.Webelievemutationtestingholdsthekey:ourkeyinsight\nistoconstructmutantsthatdenotefaultsthatarebothrelevantto\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor theissueofconcernandalsocurrentlynotcaught(unkilled)byany\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nexistingtestcase,andtousetheseaspromptstoLLM-basedtest\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation\nonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe generation.ThisresultsinanoverallageneticLLM-basedworkflow,\nauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or inwhichagentsessentiallygenerateproblem-specific‘superbugs’\nrepublish,topostonserversortoredistributetolists,requirespriorspecificpermission\nand/orafee.Requestpermissionsfrompermissions@acm.org. andtheteststhatcancatchthem.\nFSECompanion’25,23–27,2025,Trondheim,Norway\n©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.\nACMISBN979-8-4007-0658-5/24/07 2ACH(AutomatedComplianceHardener)isso-namedbecauseitisanautomated\nhttps://doi.org/10.1145/3663529.3663839 testgenerationtoolthat‘hardens’compliancewithrespecttochosenissuesofconcern.\n5202\nnaJ\n22\n]ES.sc[\n1v26821.1052:viXra\nFSECompanion’25,23–27,2025,Trondheim,Norway Harman,Sengupta\nAtMeta,wehavedevelopedanddeployedjustsuchanapproach wehaveaboutpotentialissuesoursystemswillface.Furthermore,\nandtool,ACH,forautomatedunittestgeneration.ACH’swork- becauseACHgeneratessimulatedfaultsasanintermediatestage\nflow is based on the principle of Assured LLM-based Software ingeneratingtests,wecanusethedistributionsofsimulatedfault\nEngineering[7].InAssuredLLMSE,thegoalisnotmerelytouse prevalence over the code base as a proxy for the risk exposure\nlanguagemodelstotacklesoftwareengineeringchallenges,butto ofindividualsystemcomponentstotheissueundertest.Finally,\nautomaticallygeneratesoftwareengineeringartifactsthatcome throughsimulation[2],wecanfurtherassessthelikelyimpact(or\nwithassurances.InthecaseofACH,theartifactsaretestsandthey severity)ofsuchfaults,weretheytomanifestasfieldfailures.\ncomewiththefollowingassurances: Therearefourprimarycontributionsofthepaper:\n(1) Buildable:Theproposednewtestsbuild,soarefreefrom Empirical results: We report results from the application of\nsyntaxerrorsandmissingdependencies; ACHto7realworldsoftwareplatformsdeployedbyMeta.Intotal,\n(2) ValidRegressionTests:Thetestspassanddosoconsis- ACHgenerated4,660candidatemutants(simulatedprivacyfaults)\ntently,sotheyarenon-flaky[26]regressiontests; thatit‘believed’tobepotentiallykillablefrom10,795classesunder\n(3) Hardening:Thenewtestscatchfaultsthatnoexistingtest test.Fromthesecandidates,ACHwasabletogenerateanadditional\ncancatch; 571unittests.Ofthese571tests,277wouldhavebeendiscarded\n(4) Relevant:Manyofthenewtestsarecloselycoupledtothe hadwechosentofocussolelyonthelinecoveragetestadequacy\nissueofconcern; criterion,underliningtheimportanceofmutationtestingoversuch\n(5) FashionFollowing:Mostofthenewtestsrespectthecod- coverage.\ningstyleusedbytheexistingtestsavailablefortheclass DeploymentExperience:Wereportbothqualitativeandquanti-\nundertest;theyare‘fashionfollowers’[3]. tativeoutcomesfromthedevelopment,deploymentandevaluation\nofACHatMetain2024.Overall,thetestsautomaticallygenerated\nThefirstthreeassurancesarebooleaninnature;theyareun-\nbyACHachievedanacceptancerateof73%fromtheengineers\nequivocal guarantees made by ACH about the tests it proposes\nwhoreviewedthem,with36%beingjudgedprivacyrelevant.\nto the engineer. The fourth and fifth are more aspirational and\nEquivalentMutantDetection:Wepresentanevaluationofthe\nprobabilisticinnature,reflectingtheunderlying(LLM)technol-\nACHequivalentmutantdetectionagent’sabilitytodetectequiv-\nogyusedtoconstructtests.Thedegreetowhichtestsarerelevant\nalentmutants,whichyieldsprecisionandrecallof0.79and0.47\nand‘fashionfollow’theexistingtestsisaninherentlynon-boolean\nrespectively.Thefigureforprecisionandrecallrisesto0.95and\nandsubjectiveattribute.Assuch,whilethebooleancriteriaare\n0.96 respectively, when the agent is combined with simple pre-\nprovidedasverifiableguaranteesthatACHpromisestomeet,the\nprocessing.\ntwomoreprobabilisticassurancesarebestassessedthroughthe\nLessons from Industrial Application: We detail the lessons\nstandardcodereviewprocessroutinelyundertakenbyengineers.\nlearned,andopenproblemsandresearchchallengesraisedbythis\nFinally,whenitfindsthatanewly-generatedtestaddscoverage,\napplicationofAssuredLLMSEtomutation-basedtestgeneration.\nACHalsomeasurescoverageachieved,includingtheresultinthe\nassurancesprovidedtotheengineerreviewingthetest.\n2 THEACHSYSTEM\nThenewlygeneratedtestsneednotextendcoverage,sincethey\nmayfindfaultssimplybycoveringexistinglinesofcodeinsome Figure1presentstheoverallarchitecturalpipelineoftheACHSys-\nnew way; the same path with different data, for example. It is tem.ACHstartswithfreefromtextaboutanissueofconcern.This\nwellknown[17]thatmutationtestinghasthisproperty,allowing textualinputcouldcomefromoneormoreofavarietyofsources,\nmutationtestingtoclaimsuperiorityoverstructuralcoveragetest including(butnotlimitedto):\nadequacycriteriasuchaslinecoverage.AsshowninSection6,our (1) Previousfaultsfoundindevelopment;\nfindingsfurtherunderscoretheimportanceofmutationtestingin (2) Userrequirements;\n‘goingbeyond’purelystructuraltestadequacycriteria. (3) Systemtechnicalconstraints;\nThepaperfocusesonprivacyhardening,butwebelievethat (4) Concernsraisedbyengineers,managersororganizational\nourapproachwillfindmanyapplicationstosoftwaretesting,more leadershipaboutundesirablesystembehaviours;\ngenerally. It combines existing well-known and widely-studied (5) Regulatoryrequirementssetbylegislativebodiesandcom-\napproachestomutationtesting,assuredLLMSE,andtestgeneration. plianceenforcementorganizations.\nHowever,thisnewcombinationallowsittotackleawiderangeof\nThisisthesenseinwhichACHisa‘compliancehardener’:it\notherproblems.Thepotentialreachandimpactofthisapproach\nimproves(‘hardens’)theabilityofthedeployedregressiontestin-\nderivesfromthefactthatitanswersafundamentalquestionfor\nfrastructuretodetectregressionsthatmightleadtonon-compliance\nautomatedsoftwaretestgeneration:\nwithrespecttotheissueofconcern.\nHow can we automatically transform vague, incom- Theresultspresentedinthispaperwereobtainedbyusingpri-\npleteandevencontradictorytextualnarrativesabout vacyhardeningconcernsfrompreviousfaultsfoundindevelopment.\nsoftwareconcerns,intounitteststhatguardagainst ThesinglelanguagemodelLlama3.170Bn[42]wasusedinallthe\nbugsthat,ifintroduced,mayyieldfieldfailuresthat agentsreportedon.ThepromptsusedbythethreeLLMagents\nmanifesttheseconcerns? fromFigure1canbefoundinTable1.\nThisfundamentalnaturemakestheapproachverywidelyap- Ourfocusisthedevelopment,deploymentandevaluationof\nplicable:wecanuseitinanysituationwherewehaveawayto mutation-guidedageneticworkflows.Wehavenotyetfelttheneed\ncapture,eveninthemostvagueoftextualdescriptions,concerns toextendtomoresophisticatedprompting,nortousefine-tuning,\nMutation-GuidedLLM-basedTestGenerationatMeta FSECompanion’25,23–27,2025,Trondheim,Norway\nRemove\nCurr fr e o n m t I s C s I ues su I m ss m ue a ry Pr f i a v u a l c t y Builds Yes Passes Yes Fault sy i n d t e a n c t t i i c c a a l l ly\nLLM faults LLM\nMake a fault Equivalence\nNo no detector\nyes Equivalent?\nExisting test Class under\nclass test\nDiscarded\nCurrent code repository faults\nNo\nAuto-generate\nNew tests Builds Yes Pa o s ri s g e in s a o l n Yes Pas fa s u es lt on No diff summary\nand test plan\nLLM Onward\nMake a test code\nto catch fault No No Yes re in v i C ew I\nDiscarded\nnew tests\nDotted section is a version of the TestGen-LLM workflow\nFigure1:ToplevelarchitectureoftheprincipalACHcomponents.Thedottedsectiondenotesa(slightlymodified)version\noftheTestGen-LLMtool,onwhichwepreviouslyreported[3].Theworkflowprecedingthis,shownaboveinthefigure,is\ntheadditionalageneticworkflowforgeneratingcandidatefaultstodrivethegenerationoftests.Solidrectanglesdenote\ncomponentsthatarefullyautomatedbutentirelyrule-based(andthereforedonotuseLLMs).\nTable1:ThethreesimplepromptsusedinthearchitecturediagraminFigure1\nAgentname PromptTemplate,inwhich{···}denotesaparametertothetemplate\nMakeafault CONTEXT: {context_about_concern} INSTRUCTION: Here is a Kotlin class and a test class with some unit tests for the class under test\n‘‘‘{class_under_test}‘‘‘. ‘‘‘{existing_test_class}‘‘‘. Write a new version of the class under test in which each method is replaced by a new\nversionofthatmethodthatcontainsatypicalbugthatintroducesaprivacyviolationsimilarto{diff}.Delimitthemutatedpartusingthe\ncomment-pair‘//MUTANT<START>‘and‘//MUTANT<END>‘\nEquivalencedetector I’m going to show you two slightly different versions of a Kotlin class. Here is the first version of the Kotlin class:‘‘‘class_version1‘‘‘.\nHere is the second version of the Kotlin class:‘‘‘class_version2‘‘‘. INSTRUCTION: If the first version of the class will always do exactly\nthe same thing as the second version of the class, just respond with ‘{yes}‘. However, if the two versions of the class are not equivalent,\nrespond with ‘{no}‘, and give an explanation of how execution of the first version can produce a different behaviour to execution of the\nsecondversion.\nMakeatesttocatchfault What follows is two versions of a Kotlin class under test. An original correct class and a mutated version of that class that contains one\nmutant per method, each of which represents a bug. Each bug is delimited by the comment-pair ‘// MUTANT <START>‘ and ‘// MUTANT <END>‘. The\noriginal class and its mutant are followed by a test class that contains unit tests for the original correct class under test. This is the\noriginal version of the class under test:‘‘‘{original_class}‘‘‘. This is the mutated version of the class under test:‘‘‘{mutated_class}‘‘‘.\nHereistheexistingtestclass:‘‘‘{existing_test_class}‘‘‘.Writeanextendedversionofthetestclassthatcontainsextratestcasesthat\nwillfailonthemutantversionoftheclass,butwouldpassonthecorrectversion.\nnortoexploitlanguagemodelensembles[3],allofwhichwould mutants[27].Totacklethisequivalentmutantproblem,ACHuses\nimproveontheresultswereport.Therefore,ourresultsdenote afurtheragent,theEquivalenceDetectoragent.ThisEquivalence\nonlyabaselineagainstwhichtomeasurefuturesdevelopments. DetectoragentusestheLLM-as-judgeapproach[24,63].Thejudge\nNevertheless,despitetheselimitations,webelievetheresultsclearly is instructed to determine whether the mutant is equivalent to\nhighlightthepotentialforsignificantadvancesintestgeneration;its theoriginalclassundertest.Althoughtheunderlyingproblemof\napplicabilityandeffectiveness.Wewouldbeinterestedandexcited mutantequivalenceisundecidable[35],wefoundthatthisapproach\ntocollaboratewiththewiderresearchcommunity,andhopethis wassurprisinglyeffectiveduetothepropertiesofthemutantsso-\npaperstimulatesfurtherworkinthisarea. generated(Section5containsdetails).\nIntheACHworkflowdepictedinFigure1,theissuesummary Thefaultsgeneratedbytheinitialphaseoftheworkfloware\ngeneratespromptsthatstimulateanotherLLM-basedagenttogen- usedtogeneratepromptsfortestgeneration.Thetestgeneration\neratefaults;walkingthecodebase,usingexistingtests,classes phaseshowninadottedboxisaslightlymodifiedversionofthe\nundertest,andthesummaryaspromptingredients.Theprompts previouslypublishedTestGen-LLMtool[3].\ninstructtheLLMtogeneratesimulatedfaults(akamutants[35]).\nThereisnoguaranteethatthesemutantswillchangethecodeunder\n3 RESULTSFROMMETA’SACHDEPLOYMENT\ntest,becauseitmaynotberelevanttotheissuebeinghardened.\nHowever,evenwhenthefaultgeneratordoeschangethesyntax, WeappliedACHtothe7MetaplatformsAloha,FacebookFeed,\nitmaynotchangethesemantics;themutantscouldbeequivalent Instagram,Messenger,Oculus,Wearables,andWhatsApp.Facebook\nFSECompanion’25,23–27,2025,Trondheim,Norway Harman,Sengupta\nFeed and Instagram are social media platforms. WhatsApp and\nMessengeraremessagingplatforms.Oculusisasetofvirtualreality\nheadsets.Wearablesareplatformsforaugmentedrealityglassesand\nwristcontrollers.Wealsoincluded‘cross-app’softwareproducts\nthatprovidefeaturesaffectingmorethanoneoftheseplatforms.\nTable2presentstoplevelsummarystatisticsfortheapplication\nofACHtotheseplatforms.Onceamutantisfoundthatbuildsand\npasses,theworkflowterminates3forthecurrentclassundertest. Figure2:Likertscaleinstructionsgiventocodereviewersto\nscoretestcasesaccordingtotheirprivacyrelevance.\nThenumberofcandidatemutantsreportedinTable2istherefore\nboundedabovebythenumberofclassesundertest.\nBycontrastwithtraditionalrule-basedmutationtestingtools[19,\ntechnologiessuchasOculusandWearableComputing,andalso\n33,37,54],ACHgeneratesrelativelyfew,highlyspecificmutants,\ncodethatspannedovermultipleapps.\nbydesign.Furthermore,thegeneratedmutantshaveahigherprob-\nTable3presentsdetailsofthese30diffs.Ascanbeseen,the\nabilityofrelevancetotheissueofconcernthancanbeachievedby\noverallacceptancerateis90%acceptedofthosesubmitted(27of30),\nrule-basedapproaches.\nand93%ofalldiffsreviewed(27of29).Initialresultsforacceptance\nAscanbeseenfromTable2,ACHgenerates9,095mutantsthat\nrateweredeemedtobehighlyencouraging,soweproceededto\nbuildandpassfrom10,795classesundertest.Althoughthelan-\nbuildouttheMinimalViableProduct(MVP)anduseitinthetest-\nguagemodelapproachgeneratesfewerandmorespecificmutants\na-thonsreportedinSection4.2.\nthanrule-basedapproaches,wefoundthatitalsogeneratesmore\nTheprimaryfeedbackfromdeveloperswhoreviewedthese30\nequivalentmutants.Forexample,asTable2reveals,25%ofthemu-\ndiffswasasfollows.Engineersreportedthat\ntantsgeneratedaretriviallysyntacticallyequivalent.Thiscontrasts\n(1) Manyofthetestsaddedcoverage,whichtheycomputed\nwithoverallequivalentmutantgenerationratesthataretypically\nthemselvesmanually.Theyaskedthatcoverageinformation\naround10%to15%forrule-basedapproaches[27,45].Fortunately,\nbe automatically computed and reported in the diff sum-\nasrevealedinSection5,theequivalentmutantproblemisrelatively\nmaries,aswellasthefaultsfound.\nunimportantforourusecase.\n(2) Some of the tests were relevant to privacy, and could be\nOfthe9,095mutantsthatbuildandpass,4,660(51%)weredeemed\nusefulinhardeningagainstfutureprivacyregressions.They\ntobenon-equivalentbytheworkflow,andthusbecomethesubject\nalsofeltthathavingthespecificexamplefaultswasvery\nofeachofthepromptsusedfortestgeneration.Thetestgeneration\nhelpfulinunderstandingthebehaviorofthetests.\nphaseusessimilarpromptstothosereportedpreviously[3],but\n(3) Evenforthosewhichwerenotclearlyrelatedtoprivacy,the\nbasedoncoveringthemutantratherthancoveringuncoveredlines.\ntestsseemedtobeaddingvaluebytacklingcornercasesor\naddingcoverage.\n4 ENGINEERS’EVALUATIONOFACH\nWefollowedthetried-and-testedformula[3]forthedeployment 4.2 ExperiencefromPrivacyTest-a-thons\nofanewsoftwaretestingtechnologyatMeta,startingwithinitial\nIntheweekof9thDecember2024,weconductedtwotest-a-thons,\ntrialsandmovingontototest-a-thonsledbyengineers,thereby\nfocusingontheapplicationofACHtoMeta’stwomessagingplat-\nevaluatinginitialdeployment.\nforms:WhatsAppandMessenger.\nAswiththeinitialtrial,testcasesweresubmittedasdiffsinto\n4.1 InitialTrial\nthenormalcontinuousintegrationreviewprocess.However,the\nAtMeta,acodechangesubmittedtotheContinuousIntegration(CI) diffsummaryadditionallyclaimedadditionalcoverage,forthose\nsystemiscalleda‘diff’(shortfordifferential).Inordertogetinitial teststhatdidaddcoverageaswellasfindingcurrentlyuncatchable\nfeedbackfromengineersonthetestsgeneratedbythefirstversion faults.Theengineersparticipatinginthetest-a-thonsreviewedthe\nofACH,weusedittogenerate30newtestcases,submittingeachas diffsforusefulnessinthenormalwaytheywouldanyotherdiff,\naseparatediffforreview.Thegeneratedtestswererecommended ultimatelydeterminingwhetherthediffisaccepted,andthuslands\ntoengineersinthesamewayasanyothercodemodification,such intoproduction.\nasthosecreatedbyhumanengineers.Thatis,ACHsubmitstests, Inordertoevaluatetheprivacyrelevanceofeachtest,wead-\nasdiffs,throughthenormalreviewprocess.TheACHdiffsummary ditionallyaskedthesoftwareengineerstogiveeachascoreon\nexplainsthekindoffaultcaughtbythetestcasegiving,asaspecific a Likert scale [32]. Figure 2 is a screen capture of the exact in-\nexample,themutantthatACHhasalreadydeterminedtobekilled structionsgiventotheengineersregardingthisprivacyrelevance\nbythetestcase. scoringprocedure.\nWewantedtoobtainbroadcoverageofMeta’splatforms,so ProcedureforMessengertwo-phasetest-a-thon:TheMessen-\nincludedmessagingappslikeWhatsAppandMessenger,traditional gertest-a-thonwasconductedintwophases.Thesame6reviewers\nsocialmediaplatformssuchasFacebookFeed,aswellashardware wereusedforboththefirstandthesecondphase.AllMessenger\nreviewers had strong expertise in testing, and (at least a) basic\nexperiencewithprivacyengineering.\n3Thisdecisionwasmotivatedbytheusecase:wefirstwanttoestablishwhetheratest\nInthefirstphase,50generatedtestswereselectedfromaran-\nisrelevant.Whenwefindatestthatisrelevant,wecanchoosetofocusinonthecode\nittests,hopingthatadjacentcodewillalsohaveahigherlikelihoodofrelevance. domlyselectedpoolof60tests.All60wereprescreenedmanually\nMutation-GuidedLLM-basedTestGenerationatMeta FSECompanion’25,23–27,2025,Trondheim,Norway\nTable2:MetaplatformsusedtoevaluateACHandthenumbersofsimulatedprivacyfaults‘believed’non-equivalentbythe\nequivalencedetectoragentfromFigure1.PercentagesinthefinalfourcolumnsaredistributionsofACH’sequivalence‘belief’\novermutantsthatbuildandpass,whilethoseinthefourthcolumnreportthepercentageofallmutantsthatbuildandpass.\nPlatform Numberof Numberof Numberof Numberofmutantsthatbuildandpassand...\nclasses mutants mutants are theagent forwhich theagent\nundertest generated generated syntactically believes theagent believes\nintotal thatbuild identical tobe givesno tobenon-\nandpass equivalent answer equivalent\nFacebookFeed 346 1,097 252(23%) 50(20%) 19(7.5%) 19(7.5%) 164(65%)\nMessenger 3,339 9,185 2,922(32%) 742(25%) 384(13%) 401(14%) 1,395(48%)\nInstagram 1,691 5,199 1,381(27%) 277(20%) 154(11%) 208(15%) 742(54%)\nAloha 175 576 144(25%) 32(22%) 16(11%) 6(4%) 90(63%)\nWearables 2,841 8,592 2,468(29%) 621(25%) 207(8%) 326(13%) 1,314(53%)\nCross-app 805 1,819 562(31%) 146(26%) 64(11%) 82(15%) 270(48%)\nOculus 325 997 279(28%) 96(34%) 17(6%) 16(6%) 150(54%)\nWhatsApp 1,273 4,212 1,087(26%) 282(26%) 155(14%) 115(11%) 535(49%)\nTotals 10,795 31,677 9,095(29%) 2,246(25%) 1,016(11%) 1,173(13%) 4,660(51%)\nTable3:ResultsfrominitialtrialACHdeploymenton6plat- Havingcompletedthereviewsoftheinitial72testsallocated,\nformsandcross-appproductstogaugedevelopers’reactions, severaloftheengineersrequestedadditionalteststoreview,hav-\ngainfeedbackandguidedevelopment. ingfoundtheexperiencesufficientlyrewarding.Additionaltests\nPlatform Number Statusafterhumanreviewis... werethereforeallocatedatrandomfromtheremainingpoolof48\noftests Accepted Accepted Rejected Not available.Therefore,intotal,91WhatsApptestswerereviewedfor\n‘asis’by with bythe reviewed\nusefulness,and90werereviewedforrelevance.\nthe simple engineer\nResultsfromthetest-a-thons:Table4presentssummarysta-\nengineer changes\ntisticsforthetwotest-a-thons.Theuppertablegivesthenumber\nFBFeed 4 4 0 0 0\noftestsreviewedforusefulnessandforprivacyrelevanceand,of\nMessenger 12 8 3 1 0\nthese,thenumberthatwereacceptedandrejectedforusefulness,\nAloha 2 2 0 0 0\nandscoredforrelevance.Thelowertablegivestheproportionsof\nWearables 3 1 0 1 1\nCross-app 2 2 0 0 0 testsreviewedforusefulnessthatwereacceptedandrejected,and\nOculus 3 3 0 0 0 theproportionofthosereviewedforrelevancethatfallintoeach\nWhatsApp 4 3 1 0 0 ofthefivecategoriesoftheLikertscaleonwhichrelevancewas\nassessedbythesoftwareengineers.Percentagesareroundedtothe\nTotals 30 23 4 2 1\nnearestwholenumberpercentagepoint,andsomaynotquitetotal\n100%duetorounding.\nAsTable4reveals,theoverallacceptanceratewas73%.Thisis\nand,asaresult,10wereexcludedbecausetheyconcernedonly\nverysimilartoacceptanceratesfromprevioustest-a-thonsthat\nfaultsrelatingtoNullPointerExceptions,twoofwhichimproved\nfocusedexclusivelyonelevatingcoverage[3].Overall,aboutone\ncoverage,andeightofwhichdidnot.Thesewereexcludedbecause\nthirdofthetestsdeemeddefinitelyirrelevanttoprivacy.Thismakes\nweinitially(wrongly,asitturnedout)thoughtthatsuchexception-\nitallthemoreinterestingthattherejectionrateismuchlowerthan\ncatchingtestswouldberejectedbyengineersduetobeingirrelevant\nthis,at27%.Indeed,onlyapproximately36%weredeemedtobe\ntoprivacy.However,thepre-screeningapproachwasabandoned\neitherpossiblyordefinitelyrelatedtoprivacy.Wethereforeobserve\nbasedonexperiencefromthisfirstphase:Engineersprovedready\nthattheengineersareverywillingtoaccepttestswhichmaynot\ntoaccepttests,evenwhentheywerenotdirectlyrelevant,because\nberelevanttotheircurrentconcern,whentheyarefounduseful\ntheyperceivedotherbenefits.Therefore,inthesecondphase,a\nforotherreasons.\nfurther50testswereselectedatrandom,withoutpre-screening.\nInlookingattheindividualcommentsleftbytheengineerson\nProcedurefortheWhatsApptest-a-thon:IntheWhatsApp\ntest-a-thon,72tests4wereselectedrandomlyfromapoolof120 eachofthe191teststhatwerereviewedforusefulness,weobserve,\nmoreanecdotally,thattheengineerswerelikelytoaccepttestsfor\navailableandallocatedto6engineerstoreview.Nopre-screening\ntwoprimaryreasons:\nwasperformed.All6engineershadabackgroundinbothprivacy\nengineeringandsoftwaretestingand,assuch,werewell-placed (1) Theyaddlineorbranchcoverageofnon-trivialcode\nwithrelevantexpertiseandhighlycalibratedintheirexpectations (2) Theycoveredatrickycornercase,suchashandlingspecial\naboutprivacyconcerns. values,evenwhenthisfailedtoaddcoverage\nTeststhatwererejectedtendednottoaddcoverage,ortoadd\n4Thedecisiontouse72testswasbasedontheinitialrequestfromthe6engineersthat\ncoverageonlyoftrivialcode,suchasone-linebehavioralfunctions.\ntheycouldreasonablyreview12tests;thisnumberbeingdeemedlargeenoughfor\ncalibration,yetsmallenoughtoavoidrevieweroverload. Testswerealsorejectediftheywerewritteninastylethatwas\nFSECompanion’25,23–27,2025,Trondheim,Norway Harman,Sengupta\nTable4:ResultsfromWhatsAppandMessengerTest-a-thons,whereACHwasdeployedandevaluatedinDecember2024.\ntotalnumber totalnumber totalnumber\noftests oftests oftests privacyrelevance\nreviewed thatwere... reviewed scoredatlevel...\nTest-a-thon forusefulness accepted rejected forrelevance 5 4 3 2 1\nWhatsApp 91 50 41 90 6 29 8 24 23\nMessengerPhase1(pre-screened) 50 47 3 44 5 8 5 4 22\nMessengerPhase2(notpre-screened) 50 43 7 41 4 11 0 7 19\nOverallTotal 191 140 51 175 15 48 13 35 64\nProportionsoftestsineachcategoryforusefulnessandrelevance:-\nOveralltest-a-thons 73% 27% 9% 27% 7% 20% 37%\nWhatsApp 56% 44% 7% 32% 9% 26% 26%\nMessengerPhases1and2 90% 10% 11% 22% 6% 13% 48%\nMessengerPhase1alone 94% 6% 11% 18% 11% 9% 50%\nMessengerPhase2alone 86% 14% 10% 27% 0% 17% 46%\ndeemedtobeunsuitable,suchasusingadeprecatedAPI,something 5 THEEQUIVALENTMUTANTPROBLEM\nthatwasalsoobservedinpreviouswork[3]. Allapproachestomutationtestingneedtotackletheproblemof\nIntermsoftherelevanceofthetests,webelievethatthefind- equivalentmutants[35]:themutantmaybesyntacticallydifferent\ningthat36%aredeemedrelevanttoprivacyisapositiveoverall to the original program, but we cannot guarantee it will be se-\noutcome.Thisisbecausewedevotedsolittleoftheoverallprocess manticallydifferent,becausetheunderlyingprogramequivalence\ntoweedingoutteststhatcouldbeautomaticallydeterminedtobe problemisundecidable[27,59].\nirrelevant.Assuch,theresultforrelevancecanbeconsidereda Thereareanumberoftechniquesintheliteraturethatcanweed\nbaselineforcomparisonwithfuturework.Webelievethatwith outsomeoftheequivalentmutants[40].However,theundecid-\nadditionalstaticanalysis,andfurtherLLM-as-judgeagentsinthe abilityoftheproblemmeansthatsomeequivalentmutantswill\noverallworkflow,itcouldbeconsiderablyimproved. inevitablyremain,soengineersandautomatedtestgeneratorsmay\nWealsonoticedthatthecommentsaboutrelevancefortests wastetimetryingtokill(unkillable)equivalentmutants.\nscored 4 and 2 were often quite similar, indicating uncertainty,\nand the belief that the test may be relevant to privacy, but the 5.1 EquivalentMutantsHavenoImpactonACH\nengineer was not certain. Therefore, an upper bound on those\nFortheapplicationofmutation-guidedtestgeneration,theequiva-\npotentiallyrelevanttoprivacyisapproximatelytwothirdsofthose\nlentmutantproblemhasnodirectimpactonengineers.Ourwork-\ntestsassessed.Giventhe73%acceptancerate,weconcludethat\nflowrequiresonlythatengineersreviewtestcases,notmutants.\nACHaddsprivacyhardeninginaboutonethirdofthecases,and\nTheonlyscenarioinwhichanengineermightconsiderlookingata\ndoesnotcauseunnecessarydeveloperfrictioninconsideringthe\nmutant,wouldbetoseeanexampleofthekindoffaultsthatcanbe\nremainingcases.\ncaughtbythetestcasetheyarereviewing.Byconstruction,such\nThereisaninterestingdifferenceintherelevancescorebetween\nmutantsarenon-equivalent,sotheengineerwillneverseeanequiv-\nthetwophasesoftheMessengertest-a-thonresultsreportedin\nalentmutant.Thisrelegatestheequivalentmutantproblemtoa\nTable 4. In the first phase, the engineers stated that they were\nrelativelysubordinatepositioninouroverallusecaseformutation\nunsurein10%ofthecases,butthispercentagedroppedtozero\ntesting.\ninthesecondphase.Thisapparentgrowingscoring‘confidence’\nNevertheless,itwouldbeinefficienttogeneratemanyequivalent\nindicatesapotential‘learningeffect’,whichhasbeenseeninsimilar\nmutants,becauseACHwouldwastecomputationalresourcestrying\nempiricalstudiesofsoftwareengineersduringmultiphasetrials\ntokilltheunkillable.Wethereforeincorporate,intotheagentic\n[21].\nworkflow,anLLM-basedagentthatchecksformutantequivalence.\nFinally,lookingatthedifferencesbetweentheacceptancerates\nThe remainder of this section reports on the evaluation of the\nandrelevanceassessmentsforWhatsAppandMessenger,wealso\neffectivenessofthisagent.\nsee interesting differences. WhatsApp engineers rated the tests\ntobeatleastasrelevanttoprivacy(39%forWhatsAppvs.33%\n5.2 DetectingEquivalentMutants\nforMessenger),yetacceptedfewertests(56%WhatsAppvs.89%\nformessenger).Allteststhatwereacceptableforusefulnesswere Toevaluatetheperformanceoftheequivalencedetectoragentfrom\nlandedintoproduction.Webelievethatthedifferentacceptance Figure1,weperformedamanualstudyonarandomselectionof\nratesmaysimplydenotedifferentculturesbetweendifferentteams; mutantsdrawnfromthefourplatformswiththemostmutants\ndecidingtolandatestintoproductionisaninherentlysubjective available.Thepurposeofthismanualanalysisistoanswerthe\njudgmentandmaybeinfluencedbyteamculture. researchquestion:\nHowgoodistheEquivalenceDetectorAgent?\nMutation-GuidedLLM-basedTestGenerationatMeta FSECompanion’25,23–27,2025,Trondheim,Norway\nWemustmanuallyanalyzemutantstoanswerthisresearchques- Thereasonthatthiscategoryissointeresting,isthatACHcould\ntion,becauseitrequiresagroundtruthforequivalence.However,it simplystripoutanycommentsaddedwhenconstructingamutant\nisimportanttounderlinethatitisnevernecessaryforapracticing beforesubmittingittotheequivalencedetectoragent.Thissimple\nsoftwareengineertomanuallyevaluateanymutant. pre-processingstepwouldrenderallcasesofmisleadingcomments\nFor each of the four platforms, we attempted to sample 100 syntacticallyidentical.Theywouldthenbeautomaticallydiscarded\nmutantsthatwouldstillbuildandpassontheoriginalcode.The withoutevenneedingtoconsulttheEquivalenceDetectorAgent.\nactual number manually analyzed for each sample was slightly Basedontheevaluationreportedhere,wedecidedtoincorporate\ndifferenttotheintended100,becausesomecodemayhavechanged thispre-processorintoACH’sworkflow.\nsincethemutantswereconstructed.ThisistheMutantRelevance\nProblem [43]: previously generated mutants may no longer be\n5.3 EquivalenceDetectorPrecisionandRecall\nrelevantwhenthecodechangesaftertheywereconstructed.\nMutantrelevancedoesnotimpactthedeploymentofACH,since Table6reportstheoverallprecisionandrecalloftheequivalence\nACHgeneratesmutantsonthefly,discardingthemoncetestshave detectoragent.Thedetailedresultsforeachplatformareshown\nbeengeneratedfromthem.However,itdidhaveaminorimpact inTable7.AsTable6reveals,whenweconsider‘unsure’asthe\nonourmutantsamplingtoevaluatetheresearchquestion:Since sameas‘equivalent’,theprecisionisgood,at0.79overallplatforms\nwecannotbesuremutantsinchangedcoderemainvalidatman- studied.However,recallisrelativelylow,at0.47.Ifwewantedeven\nualinspectiontime,wediscardedthemfromthehumananalysis greaterprecisionwecouldtreat‘unsure’as‘non-equivalent’,since\nsample. thisgivesprecisionof0.97(andarecallof0.44).\nInourevaluationofequivalencedetection,weareconcernedonly Basedonthisprecisionandrecall,wecanhavehighconfidence\nwithso-called‘weak’mutationtesting,not‘strong’mutationtesting that,whenthedetectordeterminesamutanttobeequivalent,itis\n[35].Thatis,becauseACHisaunittestgenerationtechnology,we verylikelytobecorrect.However,itweedsoutonlyapproximately\ndonotneedtoconsiderfailederrorpropagation[9],whichwouldbe halfoftheequivalentmutants.HighprecisionmeansACHwillnot\nrequiredforstrongmutationtesting.Failederrorpropagationisthe discardmanynon-equivalentmutants.Notlosingmutantsmeans\nsituationinwhichmutantexecutionchangesthelocalcomputation notlosingtheteststhatACHmightgeneratefromthem.Bycontrast,\nstate, yet this change is always masked along every path to an higherrecallwouldmerelysavesomecomputationalresource.\nobservableoutput.Inweakmutationtesting,themutantisnon- Assuch,highprecisionisgenerallymorevaluablethanhigh\nequivalentifthelocalstateischanged,irrespectiveofthewhether recallforourusecase,becausewearepreparedtospendcomputa-\nthechangepropagates.Manuallydetectingfailederrorpropagation tionalresourcestoautomaticallygenerategoodunittests.However,\nisahighlylabor-intensiveprocess,sowearefortunatethatitisnot thelowertherecall,themoreoftenACHwillinefficientlyseekto\nrequired. ‘killtheunkillable’,sowestillwantthehighestrecallachievable\nTable5describesthetypesofmutant,basedonourmanualanal- withoutreducingprecision.\nysis.Inthisanalysis,theterms‘deletion’and‘injection’eachrefer AsTable2shows,approximately25%ofallmutantsaretrivially\ntosemanticchangeswherethesolechangeis,respectively,the equivalent,beingsyntacticallyidenticaltotheoriginal.Further-\nadditionofvaluesortheremovalofvalues.Theterm‘Injection more,Table5revealsthat61%ofallequivalentmutantsarealmost\nanddeletion’referstocaseswherethesemanticsarechangedby triviallyequivalent,becausetheycontainonlyasingle‘misleading’\nanindependentcombinationofadditionandremoval.Thecate- comment.Syntacticallyidenticalmutantsareremovedbyasimple\ngory‘other’coverscaseswherethechangeismorenuanced.For lexicalcomparison,whileasimplepre-processingtransformation\nexamplereplacingonevariablenamewithanother,whichcouldbe toremovecommentswouldremovethe‘misleadingcomments’\ncountedasadeletionandaninjection,butwherethetwoarenot category.Therefore,wecancombinetheLLM-baseddetectorwith\nindependent. asimplerulebasedpre-processortoyieldanoverallprecisionand\nInTable5,thecolumn‘equivalenceisnotobvious’countsthe recallof0.95and0.96respectively(SeeTable6).\nnumberofcaseswherethehumanassessorhadtothinknoticeably Thesefiguresforprecisionandrecallaresurprisinglyhigh,given\nlonger about whether the mutant was equivalent. For all other thattheunderlyingproblemisundecidable.However,wecannot\ncases,thedecisionwasobvious.AsTable5reveals,thedecisionis claimthatthesesurprisinglygoodresultsarisebecausethemutant\n‘obvious’forasurprisinglylargenumberofmutants. equivalencedetectoragentisexcellentatdeterminingprogram\nThecategory‘Misleadingcomment’isperhapsthemostinterest- equivalence.Rather,theyaremorereflectionofthekindofmutants\ningcategory.Thesearecaseswherethemutantisclearlyequivalent, thedetectorneedstojudgeforequivalence.Thatis,themutant\nbecauseallthemutantgenerationagenthasdoneistoinsertcom- generationagenttendstomakechangesthateitherobviouslyintro-\nments;noexecutablecodehasbeenadded.Anexampleofsucha ducesemanticchanges,orareobviouslyequivalent(suchasthose\nmisleadinginjectedcommentis:“// Introduce a bug by not thatonlychangecomments).\nchecking if the user is inactivated before resetting\ntheir custom reactions”.Sincelanguagemodelsaretext-based\n6 THEIMPORTANCEOFMUTATIONTESTING\npredictivetechnologiesthatcurrentlydonotspecificallydistin-\nguishexecutablecodefromcomments,suchmisleadingcomments It is well known in the literature on testing, both theoretically\nareclearlyproblematic.Theycaneasilymisleadtheequivalence andempirically,thatmutationadequacycriteriaoutperformtradi-\ndetectoragentintopredictingthatamutantisnotequivalent,when tionalstructuralcriteria,suchaslinecoverageandbranchcoverage\nitclearlyisequivalent. [17,46].Inthissection,wepresentresultsthatfurtherunderline\nFSECompanion’25,23–27,2025,Trondheim,Norway Harman,Sengupta\nTable 5: Number of mutants of different semantic types generated. We used human analysis to determine ground truth\nequivalence,andthedifferenttypesofsemantictransformationusedtogenerateamutants.\nmutantequivalence semantictypeofmutationtransformation\nPlatform Total Ground LLMagent Equivalence Deletion Injection Deletion Misleading Other\nmutants truth claims isnot and comment\nanalyzed equivalent equivalence obvious injection\nMessenger 91 38(42%) 27(30%) 2(2%) 23(25%) 32(35%) 19(21%) 11(12%) 6(7%)\nWearables 101 32(32%) 11(12%) 3(3%) 26(26%) 24(24%) 20(20%) 24(24%) 7(7%)\nWhatsApp 99 30(30%) 16(16%) 3(3%) 29(29%) 26(26%) 16(16%) 26(26%) 2(2%)\nInstagram 90 37(41%) 12(13%) 2(2%) 27(30%) 14(16%) 11(12%) 32(36%) 6(7%)\nOverallTotals 381 137(36%) 66(17%) 10(2.6%) 105(28%) 96(25%) 66(17%) 93(24%) 21(6%)\nTable6:Precision&recallfortheequivalencedetectoragent theexistingliteratureandaddweighttoclaimsfortheimportance\nwhenunsureissimplynotcounted ofmutationtesting,basedonindustrialpracticeandexperience.\nTP FP TN FN Precision Recall It is also worth noting that 70% of the mutants left unkilled\n56 2 205 72 0.97 0.44 byTestGen-LLMresideinclassesthatdonothaveanyTestGen-\nwithunsurecountedasequivalent LLMtest,letaloneonethatkillsthemutant.Combinedwiththe\nTP FP TN FN Precision Recall highproportion(51%)ofACHteststhatdoalsoraisecoverage,we\n65 17 205 72 0.79 0.47 concludethatMutation-GuidedLLM-basedTestGenerationhas\nattractivesidebenefitsoncoverage.\nwithidenticalcodecountedasequivalent\nSpecifically,wemaybeabletoadaptourACHapproachtotarget\nTP FP TN FN Precision Recall\ncoverageusingmutants:Supposewegeneratemanymutants,of\n161 17 205 72 0.90 0.69\nall different kinds, and at all different levels of abstraction, for\nwiththestrippingofalladdedcomments a given method under test. We can then use each as a prompt\nTP FP TN FN Precision Recall toanLLM.Inthisway,mutantsplayarolesimilartoRetrieval\n183 9 251 8 0.95 0.96 AugmentedGeneration(RAG).Givenourresultsandthefactthat\nRAGisknowntoimproveLLMperformanceonotherSoftware\nEngineeringtasks[22],webelievemutation-as-RAGforcoverage\nthesepreviousresearchfindingswithdirectexperiencefromthe islikelytoproveattractiveforcoverageaswellasfortargeting\ndeploymentofmutationtestingonlargescaleindustrialsystems. specificfaults.\nTable8reportslinecoverageresultsfortheteststhatkillnon-\nequivalent mutants. For Messenger, Instagram, Wearables and\nWhatsAppthe‘likelyactual’numberisbasedonthegroundtruth\n7 RELATEDWORK\nequivalentproportionfortheseplatformsinTable5.Fortheother\nplatforms,itisbasedontheoverallaveragegroundtruthproportion Giventhestrongempiricalevidenceforthepredictabilityofcode\noverallfourplatforms.Thefinalthreecolumnsshowtheresults [10,23,28],itisunsurprisingthatpredictivelanguagemodelshave\nforTestGen-LLM[3]generatedtests,asproportionsforcompari- provedeffectiveatgeneratingusablecode.Asaresult,LLMsnow\nsonwiththecorrespondingproportionsforACHgeneratedtests. playacode-generationroleinasignificantnumberofapplications\nTestGen-LLMdoesnottargetspecificfaults,butmerelyattempts acrossthespectrumofsoftwareengineeringactivity[22].\ntogenerateteststhatwillacquireextracoverage. Softwareengineers’acceptanceratesforLLM-generatedcode\nAsTable8shows,alargeproportion(49%)oftestcasesthat havebeenwidelystudied.Forexample,researchersatGooglefo-\nuniquelyadditionallykillamutant,donotalsoaddlinecoverage. cussed on factors that may influence trust in AI-powered code\nClearly,thesetestshavevalue,sincetheycatchfaultsthatwould completions[13],whileitwasrecentlyreportedthatCopilothad\notherwisegoundetected.However,ifweweretojudgetestcases anacceptancerateofapproximately30%[55].\nsolelyonthebasisofthecoveragetheyadd,thensuchvaluabletest However,suchstudiesconcernthecodecompletionusecase,\ncaseswouldbewronglydiscarded. whichdoesnotcomewithanyassurances(thecodecompletions\nTheresultsinTable8alsorevealthat,althoughTestGen-LLM maynotevencompile).Bycontrast,ACHusesAssuredLLM-Based\ngenerates tests for a higher proportion of classes compared to SoftwareEngineering(AssuredLLMSE)[7].Thatis,ACHprovides\nACH(32%vs.5.3%),itnevertheless,succeedsinkillingafarsmaller assurancesaboutthesemanticsandusefulnessofthetestsitpro-\nproportionofmutants(2.4%vs15%).ThisisbecauseACHspecifi- poses.Itsproposalsarealsowholecompilablecodeunits(notmerely\ncallytargetsthemutants,whereasTestGen-LLMdoesnot. completionfragments),anditisdeployedon-demand(togener-\nFromtheseresults,weconcludethattargetingmutantscanalso ateteststargetingclassesoffaultsofparticularinterest),rather\nelevatecoverage,buttargetingcoveragewillbeinadequatetokill thanopportunistically(tosuggestcodecompletions).Thefactthat\nmutants.Althoughthisisalreadyknownfromtheresearchliter- ACHprovidestheseassurances,anditsdifferentdeploymentroute,\nature[17,46],thatliteratureisbasedonlaboratorycontrolledex- meanthatwecanexpectahigheroverallacceptanceratefrom\nperimentswithnon-industrialsystems.Ourfindingsthusaugment ACH,thanwewouldforcodecompletiontechnologies.\nMutation-GuidedLLM-basedTestGenerationatMeta FSECompanion’25,23–27,2025,Trondheim,Norway\nTable7:Precision(Prec.)andRecallachievedbytheequivalencedetectoragentforthefourplatformswithmostmutants\nMessengerMutants WearablesMutants WhatsAppMutants InstagramMutants\nWhentheagentis‘unsure’,thisissimplynotcountedindeterminingprecisionandrecall\nTP FP TN FN Prec. Recall TP FP TN FN Prec. Recall TP FP TN FN Prec. Recall TP FP TN FN Prec. Recall\n17 2 50 19 0.89 0.47 11 0 55 18 1.0 0.38 16 0 51 11 1.0 0.59 12 0 49 24 1.0 0.33\nWhentheagentis‘unsure’,thisiscountedasifithaddeterminedthemutanttobeequivalent\nTP FP TN FN Prec. Recall TP FP TN FN Prec. Recall TP FP TN FN Prec. Recall TP FP TN FN Prec. Recall\n19 3 50 19 0.86 0.50 14 1 55 18 0.93 0.50 19 9 51 11 0.68 0.63 13 4 49 24 0.76 0.35\nWhenweincludemutantsthataresyntacticallyidenticalinthenumberscountedasdeterminedtobeequivalent\nTP FP TN FN Prec. Recall TP FP TN FN Prec. Recall TP FP TN FN Prec. Recall TP FP TN FN Prec. Recall\n41 3 50 19 0.93 0.58 42 1 55 18 0.98 0.70 47 9 51 11 0.84 0.81 31 4 49 24 0.86 0.56\nWhenweadditionallyinclude,asdeterminedtobeequivalent,thosemutantsthataresyntacticallyidenticalafterstrippinganycommentsaddedbythegenerationagent\nTP FP TN FN Prec. Recall TP FP TN FN Prec. Recall TP FP TN FN Prec. Recall TP FP TN FN Prec. Recall\n41 3 67 2 0.93 0.95 42 1 69 4 0.93 0.91 47 1 66 0 0.99 1.0 53 4 49 2 0.93 0.96\nTable8:Thenumbersofteststhataddcoverageaswellasdetectingmutatedfaults.Thisgivesanupperboundonthenumber\noffaultswemightmissbymerelytargetingnewcoveragealone.Approximatelyhalfofalltestsgenerated,althoughfinding\nfaultsmissedbyallexistingtests,donotaddlinecoverage.Hadweusedcoverageasoursoletestadequacycriterion,the\nplatformswouldthushavecontinuedtobevulnerabletoregressionsdenotedbyuptoapproximatelyhalfofthefaults.\nPlatform Numberof Numberof %age %age Number %age %age %age\n(believed; mutant- ofclasses mutant (%age) Classes TestGen- unkilled\nlikelyactual) killing witha killrate oftests witha LLMkill mutants\nnon-equivalent tests mutant- on(believed; thatdo TestGen- rateon withno\nmutants killing likelyactual) notadd LLMtest (believed; TestGen-\ntest coverage likelyactual) LLMtest\nFacebookFeed 164;133 15 4.3% 9%;11% 9(60%) 33% 3.0%;3.7% 71%\nMessenger 1,395;1,155 196 5.9% 14%;17% 84(43%) 31% 2.0%;2.4% 71%\nInstagram 742;687 122 7.2% 16%;18% 77(55%) 27% 2.9%;3.1% 74%\nAloha 90;74 9 5.1% 10%;12% 4(44%) 27% 1.0%;1.2% 76%\nWearables 1,314;1,007 45 1.6% 3%;4% 32(71%) 31% 1.9%;2.5% 70%\nCross-app 270;275 35 4.3% 13%;13% 20(57%) 28% 2.8%2.7% 75%\nOculus 150;121 14 4.3% 9%;12% 3(21%) 35% 4.9%;6.1% 70%\nWhatsApp 535;445 135 10.6% 25%;30% 48(36%) 44% 3.5%;4.2% 55%\nTotals 4,660;3,897 571 5.3% 12%,15% 277(49%) 32% 2.0%;2.4% 70%\nWhilecodecompletionsaredeployeddirectlyintotheIDEin directionthatadditionallyseekstogenerateasetoffaults,relevant\nrealtime(online),ACHisanofflinetechnologythatproposescode toachosenissueofconcern.\nupdatesinexactlythesamewaythatengineerswouldpropose Mutationtestinghasalsobeenusedasawaytoevaluatelan-\nthem:throughContinuousIntegration(CI)andstandardcodere- guagemodelsthemselves[38]andtomutatecompilerinputsto\nview.Thishastheadvantagethatthedecisiontoacceptmachine- testcompilers[31].Inthiswork,thedesirablepropertyofmutants\ngeneratedcodehasanaudittrail,whichismissingforLLM-based is the way that they generate near neighborhoods of programs\ncodecompletions.Bycontrast,LLM-basedcodecompletionsugges- thataresimilartotheprogramthattheymutate,bothsyntactically\ntionsacceptedbyengineersareattributedtotheengineerbythe andsemantically.Thisallowsresearcherstoexplorepropertiesof\nCIaudittrail;detailsoftheLLM’sroleinconstructingproduction language models when applied to code, such as their ability to\ncodethroughcompletionsuggestionsisthusunavailable. ‘understand’thecode.This‘nearneighborhood’propertyhasalso\nThereisalreadyaconsiderablebodyofliteratureonLLMSE[22]. beenusedincombinationwithlanguagemodels[14]togenerate\nThesubsetofthisliteraturedevotedspecificallytosoftwaretesting mutantstohelpimproveothersoftwareengineeringtechnologies,\nistoolargetosurveyindetailhere.Instead,wereferreadersto suchasGeneticImprovement[47].\nrecentsurveys[22,61].Inthissection,wefocusspecificallyonthe Theequivalentmutantproblemhasbeenwidelystudied[35,40],\napplicationofLLMstomutationtesting. andwearenotthefirsttoconsiderusinglanguagemodelstotackle\nTraditionally[35],mutationtestingtechniqueshaveusedsetsof it. Tian et al. [56] recently studied the use of language models\npre-definedrule-basedoperatorstoconstructmutants.However,it todetectequivalentmutantsfortraditionalrule-basedmutation\nhasbeenwidelyobserved[36,51]thatsuchpre-definedrule-based testingoperatorsonMutantBench.MutantBench[59]isasetof\nmutationoperatorsareill-suitedtothetaskofgeneratingrealistic mutant-original-codepairs,writteninC/C++andJava,introduced\nfaults.Thishasledresearcherstoconsiderwaystomakemore in2021byvanHijfteandOprescu.Tianetal.foundthatLLM-based\nrealisticmutants[34],aproblemtowhichlanguagemodelshave equivalencedetectionoutperformedtraditionalnon-LLM-based\nrecentlybeenapplied[57,60].ACHrepresentsanextensionofthis equivalence detection by 35% for F1 score (the harmonic mean\nFSECompanion’25,23–27,2025,Trondheim,Norway Harman,Sengupta\nofprecisionandrecall).Inparticular,theyhighlightedthehigh- aftersomechangeifthetestbehavesthesamebeforeandafterthe\nperformanceoffine-tunedembeddings. change.Thisallowsustoprotectagainstfutureregressions,butit\nAllthepreviousworkonLLMsformutationtestinghascon- cannotdetectexistingfaultsresidinginthecodebase.Todothis\ncernedbenchmarkproblemsandempiricalanalysisunderlabora- requiresustotacklethewellknownOracleproblem[11].\ntoryconditions.Thepresentpaperisthefirsttoreportthedeploy- Itwouldbeveryexcitingifanapproachcanbefoundtoinfer\nmentofLLM-basedmutationtestingatscaleinindustry.Non-LLM- oraclesforTargetedLLM-basedMutation-GuidedTestGeneration.\nbasedmutationtestinghaspreviouslybeenevaluatedinindustrial Suchanadvancewouldbehighlyimpactfulbecauseitwouldallow\napplicationsettings,atMeta[12]andatGoogle[48–50].However, ustosearchforexistingclassesoffaults.Althoughtherehasbeen\ninboththesepreviousindustrialdeployments,therewasnoattempt muchpreviousworkonoracleinference[30,44,58,62],weneed\ntoautomaticallygenerateteststokillthemutants.Thechallenging higherprecisiontoavoidwastingengineers’timeonfalsepositives.\ntaskofconstructingteststokillmutantswaslefttoengineers,who Generatingfaults/testsforspecificissuesofconcern,asACHdoes,\nalsothereforehadtocontendwiththeequivalentmutantproblem. mayhelporaclegeneration.\nUnlikethesepreviousindustrialdeployments,ACHisalsothefirst\ntoautomaticallygenerateteststokillmutants;aproblemwhichhas 9 CONCLUSIONS\nbeenstudiedintheresearchliterature[16,25],butnotpreviously Mutationtestinghasbeenthesubjectofresearchforfivedecades\ndeployedatscaleinindustry. [20,29,35,46],sohasclearlyretainedintellectualappealandre-\nTheworkonACHreportedhere,followsasuccessionofauto- searchers’curiosity.Despitethis,ithasprovedchallengingtodeploy\nmatedanalysisandtestingworkdeployedatMeta.Ourinitialfocus mutationtestinginindustry[4,6,12].\nwasend-to-endtestgenerationtoolssuchasSapienz[5,41],and Traditionally,mutantshavebeenconstructedusingsimplerule-\nstaticanalysistoolssuchasInfer[15],followedbysimulation-based basedapproachesinordertoassesstestsuites,largelywrittenby\ntestgenerationfortestinginteractingusercommunities[1,2,58]. humans.However,softwareengineersneedautomaticallygener-\nWhilemuchofthispreviousworkhasbeenconcernedwithsystem atedtests,thatarerelevanttoaspecificpressingconcern;their\nleveltesting,wehavealsoworkedonunitleveltestgeneration timewouldbebetterspentarticulatingthesepressingconcerns,\ntechniquesusingobservations[8]andlanguagemodels[3].How- ratherthantryingtoconstructtestcasesthatshould,instead,be\never,thispreviousunittestgenerationworkwasbasedonthesole generatedbyamachine.Fortunately,twocrucialrecentadvances,\nobjectiveofincreasingcoverage,whichmeantitcouldnottarget drawntogetherinACH,makethispossible:Automatedtestgen-\nspecificclassesoffaultsasACHdoes. erationtokillgeneratedmutants,coupledwithlanguagemodels’\nabilitytogeneratehighly-relevantmutants.Inparticular,wefound\nthatLLMshelpustoovercomeexistingbarriers[12]todeployment\n8 OPENPROBLEMS\nofmutationtesting.Specifically,they\nWe outline directions for future work, hoping to stimulate the\n(1) Allowustogeneratehighlyrealisticfaults,usingtheirability\nresearchcommunitywithnewopenresearchquestions.\ntoconverttext(theconcernorissue)intocode(simulated\nMutant Equivalence: we found that the simulated privacy\nfaultsfromwhichwegeneratetests);\nfaultsgeneratedbylanguagemodelstendtobebimodal,withthe\n(2) Provideanadditionalagenttoweedoutequivalentmutants,\nconsequencethatequivalentmutantsaresurprisinglyeasytodetect\nwhichisespeciallypowerfulwhencombinedwithsimple\n(SeeSection5).Itispossiblethattheseresultsaremerelyspecific\nstaticanalysesasapre-processingstep;\ntoKotlin,toAndroid,toMeta,ortosimulatedprivacyfaults.\n(3) Provideawaytoautomaticallygenerateunitteststokillthe\nHowever,thereisnothinginourapproachthatsuggeststhat\nmutants.\ntheresultswouldfailtogeneralize.Iftheyweretogeneralize,this\nwouldbeanimportantfindingforthemutationtestingresearch ThispaperpresentedresultsfromdeploymentofACHatMeta.\nagenda,giventhesignificantimpedimenttodeploymenthitherto NeitherLLM-basedtestgeneration,norLLM-basedmutantgenera-\nposedbymutantequivalence. tionisnew,butthispaperisthefirsttoreportontheircombined\nMutantRelevance:Wehavebeenabletogeneratespecificmutants deploymentonlargescaleindustrialsystems.Webelievethisform\nthatcapturesimilarfaultybehaviorsasthosepreviouslywitnessed. ofautomatedtestgenerationishighlyalignedwithmodernsoft-\nHowever,insomecases,anecdotally,lookingatthefaultsgenerated, waredevelopmentanddeployment.Itsupportssoftwareengineers\nitseemedthatthemutantwasrelatedtothegeneralclassoffault whomustcontendwithmanycompetingandconflictingconcerns,\nsimulated,butwasnotanexampleofthespecificinstance.One oftenexpressedinnaturallanguageinvague,incompleteandeven\ndifficultyhereisthatwehavenowaytoconsistentlyandreliably contradictoryways.OurresultsalsosuggestMutation-as-RAGwill\nmeasureproblemsimilarityorrelevance. proveimpactfulinoptimizingforstructuralcoveragecriteria.\nMoreresearchisneededtodefinewhatitmeansforonefaultto\nACKNOWLEDGMENTS\nbesimilartoanother.Wealsoneedapproachesthatusesuchsimilar-\nitymetricstoguideageneticLLMworkflows,e.g.,withfine-tuning, WewouldliketothanktheMeta’sLlamateamandleadershipofthe\npromptengineering,re-prompting,and/orChain-of-Thought,to FundamentalArtificialIntelligenceResearch(FAIR),DeveloperIn-\nensurethatthemutantsgeneratedarerelevanttotheoriginalfault. frastructure(DevInfra)andProductComplianceandPrivacyteams\nDetectingexistingfaults:ourapproachhardensagainstfuture forsupportingthisworkandtheMetaSoftwareEngineerswhoso\nregressions.Thismeansthatthecurrentversionofthesystemunder freelyandkindlygaveoftheirtime,experience,andexpertisein\nthetestisusedastheregressionoracle[6]:atestisdeemedtopass reviewingthetestsautomaticallygeneratedbyACH.\nMutation-GuidedLLM-basedTestGenerationatMeta FSECompanion’25,23–27,2025,Trondheim,Norway\nREFERENCES\nSoftwareEngineering.572–576.\n[1] JohnAhlgren,MariaEugeniaBerezin,KingaBojarczuk,ElenaDulskyte,Inna [19] HenryColes,ThomasLaurent,ChristopherHenard,MikePapadakis,andAn-\nDvortsova,JohannGeorge,NatalijaGucevska,MarkHarman,MariaLomeli,Erik thonyVentresque.2016. Pit:apracticalmutationtestingtoolforJava.InPro-\nMeijer,SilviaSapora,andJustinSpahr-Summers.2021. TestingWebEnabled ceedingsofthe25thinternationalsymposiumonsoftwaretestingandanalysis.\nSimulationatScaleUsingMetamorphicTesting.InInternationalConferenceon 449–452.\nSoftwareEngineering(ICSE)SoftwareEngineeringinPractice(SEIP)track.Virtual. [20] RichardA.DeMillo,RichardJ.Lipton,andFrederickG.Sayward.1978.Hintson\n[2] JohnAhlgren,KingaBojarczuk,SophiaDrossopoulou,InnaDvortsova,Johann testdataselection:Helpforthepracticalprogrammer.IEEEComputer11(1978),\nGeorge,NatalijaGucevska,MarkHarman,MariaLomeli,SimonLucas,Erik 31–41.\nMeijer,SteveOmohundro,RubmaryRojas,SilviaSapora,JieM.Zhang,andNorm [21] JoséJavierDolado,MarkHarman,MariCarmenOtero,andLinHu.2003.An\nZhou.2021.Facebook’sCyber–CyberandCyber–PhysicalDigitalTwins(keynote empiricalinvestigationoftheinfluenceofatypeofsideeffectsonprogram\npaper).In25thInternationalConferenceonEvaluationandAssessmentinSoftware comprehension.IEEETransactionsonSoftwareEngineering29,7(2003),665–670.\nEngineering(EASE2021).Virtual. [22] AngelaFan,BelizGokkaya,MityaLyubarskiy,MarkHarman,ShubhoSengupta,\n[3] NadiaAlshahwan,JubinChheda,AnastasiaFinegenova,MarkHarman,Alexan- ShinYoo,andJieZhang.2023.LargeLanguageModelsforSoftwareEngineering:\ndruMarginean,ShubhoSengupta,andEddyWang.2024.Automatedunittest SurveyandOpenProblems.InICSEFutureofSoftwareEngineering(FoSE2023).\nimprovementusingLargeLanguageModelsatMeta.InACMInternationalCon- [23] MarkGabelandZhendongSu.2010.AStudyoftheUniquenessofSourceCode.\nferenceontheFoundationsofSoftwareEngineering(FSE2024)(PortodeGalinhas, InFSE.147–156.\nBrazil,Brazil). [24] JiaweiGu,XuhuiJiang,ZhichaoShi,HexiangTan,XuehaoZhai,ChengjinXu,\n[4] NadiaAlshahwan,AndreaCiancone,MarkHarman,YueJia,KeMao,Alexandru WeiLi,YinghanShen,ShengjieMa,HonghaoLiu,etal.2024. ASurveyon\nMarginean,AlexanderMols,HilaPeleg,FedericaSarro,andIlyaZorin.2019. LLM-as-a-Judge.arXivpreprintarXiv:2411.15594(2024).\nSomechallengesforsoftwaretestingresearch(keynotepaper).InProceedingsof [25] MarkHarman,YueJia,andWilliamB.Langdon.2011. StrongHigherOrder\nthe28thACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis\nMutation-BasedTestDataGeneration.In8𝑡ℎEuropeanSoftwareEngineering\n(ISSTA2019),Beijing,China,July15-19,2019,DongmeiZhangandAndersMøller ConferenceandtheACMSIGSOFTSymposiumontheFoundationsofSoftware\n(Eds.).ACM,1–3. Engineering(ESEC/FSE’11)(Szeged,Hungary).ACM,NewYork,NY,USA,212–\n[5] NadiaAlshahwan,XinboGao,MarkHarman,YueJia,KeMao,AlexanderMols, 222.\nTaijinTei,andIlyaZorin.2018.DeployingSearchBasedSoftwareEngineering [26] MarkHarmanandPeterO’Hearn.2018.FromStart-upstoScale-ups:Opportu-\nwithSapienzatFacebook(keynotepaper).In10𝑡ℎ InternationalSymposium nitiesandOpenProblemsforStaticandDynamicProgramAnalysis(keynote\nonSearchBasedSoftwareEngineering(SSBSE2018).Montpellier,France,3–45.\npaper).In18𝑡ℎIEEEInternationalWorkingConferenceonSourceCodeAnalysis\nSpringerLNCS11036. andManipulation(SCAM2018).Madrid,Spain,1–23.\n[6] NadiaAlshahwan,MarkHarman,andAlexandruMarginean.2023. Software [27] MarkHarman,XiangjuanYao,andYueJia.2014. AStudyofEquivalentand\nTestingResearchChallenges:AnIndustrialPerspective(keynotepaper).In2023\nStubbornMutationOperatorsUsingHumanAnalysisofEquivalence.In36𝑡ℎ\nIEEEConferenceonSoftwareTesting,VerificationandValidation(ICST2023).IEEE, InternationalConferenceonSoftwareEngineering(ICSE2014).Hyderabad,India,\n1–10. 919–930.\n[7] NadiaAlshahwan,MarkHarman,AlexandruMarginean,ShubhoSengupta,and [28] AbramHindle,EarlBarr,ZhendongSu,PremDevanbu,andMarkGabel.2012.On\nEddyWang.2024.AssuredLLM-BasedSoftwareEngineering(keynotepaper). theNaturalnessofSoftware.InInternationalConferenceonSoftwareEngineering\nIn2𝑛𝑑. ICSEworkshoponInteroperabilityandRobustnessofNeuralSoftware (ICSE2012).Zurich,Switzerland.\nEngineering(InteNSE)(Lisbon,Portugal). [29] WilliamE.Howden.1985. TheoryandPracticeofFunctionalTesting. IEEE\n[8] NadiaAlshahwan,MarkHarman,AlexandruMarginean,andEddyWang.2024. Software2,5(Sept.1985),6–17.\nObservation-basedunittestgenerationatMeta.InFoundationsofSoftwareEngi- [30] AliRezaIbrahimzada,YigitVarli,DilaraTekinoglu,andReyhanehJabbarvand.\nneering(FSE2024). 2022.Perfectistheenemyoftestoracle.InProceedingsofthe30thACMJoint\n[9] KellyAndroutsopoulos,DavidClark,HaitaoDan,MarkHarman,andRobert EuropeanSoftwareEngineeringConferenceandSymposiumontheFoundationsof\nHierons.2014.AnAnalysisoftheRelationshipbetweenConditionalEntropyand SoftwareEngineering.70–81.\nFailedErrorPropagationinSoftwareTesting.In36𝑡ℎInternationalConference [31] DavideItalianoandChrisCummins.2025.Findingmissedcodesizeoptimizations\nonSoftwareEngineering(ICSE2014).Hyderabad,India,573–583. incompilersusingLLMs.InCompilerConstruction. Toappear.\n[10] EarlT.Barr,YuriyBrun,PremkumarDevanbu,MarkHarman,andFederica [32] AndrewTJebb,VincentNg,andLouisTay.2021.AreviewofkeyLikertscale\nSarro.2014.ThePlasticSurgeryHypothesis.In22𝑛𝑑ACMSIGSOFTInternational developmentadvances:1995–2019.Frontiersinpsychology12,637547(2021).\n[33] YueJiaandMarkHarman.2008. Milu:ACustomizable,Runtime-Optimized\nS C y h m in p a o , s 3 iu 06 m –3 o 1 n 7 t . heFoundationsofSoftwareEngineering(FSE2014).HongKong, HigherOrderMutationTestingToolfortheFullCLanguage.In3𝑟𝑑 Testing\n[11] EarlT.Barr,MarkHarman,PhilMcMinn,MuzammilShahbaz,andShinYoo. AcademiaandIndustryConference-PracticeandResearchTechniques(TAIC\n2015.TheOracleProbleminSoftwareTesting:ASurvey.IEEETransactionson PART’08).Windsor,UK,94–98.\nSoftwareEngineering41,5(May2015),507–525. [34] YueJiaandMarkHarman.2009. HigherOrderMutationTesting. Journalof\n[12] MoritzBeller,Chu-PanWong,JohannesBader,AndrewScott,MateuszMachalica, InformationandSoftwareTechnology51,10(2009),1379–1393.\nSatishChandra,andErikMeijer.2021.Whatitwouldtaketousemutationtesting [35] YueJiaandMarkHarman.2011.AnAnalysisandSurveyoftheDevelopmentof\ninindustry—astudyatFacebook.In2021IEEE/ACM43rdInternationalConference MutationTesting.IEEETransactionsonSoftwareEngineering37,5(September–\nonSoftwareEngineering:SoftwareEngineeringinPractice(ICSE-SEIP).IEEE,268– October2011),649–678.\n277. [36] MatthieuJimenez,ThierryTitcheuChekam,MaximeCordy,MikePapadakis,\n[13] AdamBrown,SarahD’Angelo,AmbarMurillo,CieraJaspan,andCollinGreen. MarinosKintis,YvesLeTraon,andMarkHarman.2018. Aremutantsreally\n2024.IdentifyingtheFactorsthatInfluenceTrustinAICodeCompletion.In1st natural?:astudyonhow\"naturalness\"helpsmutantselection.InProceedingsof\nACMInternationalConferenceonAI-poweredSoftware(AIware2024)(Portode the12thACM/IEEEInternationalSymposiumonEmpiricalSoftwareEngineering\nGalinhas,Brazil). andMeasurement,ESEM2018,Oulu,Finland,October11-12,2018,MarkkuOivo,\n[14] AlexanderBrownlee,JamesCallan,KarineEven-Mendoza,AlinaGeiger,Carol DanielMéndezFernández,andAudrisMockus(Eds.).ACM,3:1–3:10.\nHanna,JustynaPetke,FedericaSarro,andDominikSobania.2023.Enhancing [37] RenéJust.2014.TheMajormutationframework:Efficientandscalablemutation\ngeneticimprovementmutationsusinglargelanguagemodels.InInternational analysisforJava.InProceedingsofthe2014internationalsymposiumonsoftware\nSymposiumonSearchBasedSoftwareEngineering.Springer,153–159. testingandanalysis.433–436.\n[15] C.Calcagno,D.Distefano,J.Dubreil,D.Gabi,P.Hooimeijer,M.Luca,P.W. [38] ZiyuLiandDonghwanShin.2024. Mutation-basedconsistencytestingfor\nO’Hearn,I.Papakonstantinou,J.Purbrick,andD.Rodriguez.2015. Moving evaluatingthecodeunderstandingcapabilityofLLMs.InProceedingsofthe\nFastwithSoftwareVerification.InNASAFormalMethods-7thInternational IEEE/ACM3rdInternationalConferenceonAIEngineering-SoftwareEngineering\nSymposium.3–11. forAI.150–159.\n[16] FranciscoCarlos,MikePapadakis,ViníciusDurelli,andEduardoMárcioDela- [39] KaiboLiu,YiyangLiu,ZhenpengChen,JieMZhang,YudongHan,YunMa,Ge\nmaro.2014.Testdatagenerationtechniquesformutationtesting:Asystematic Li,andGangHuang.2024.LLM-PoweredTestCaseGenerationforDetecting\nmapping.InWorkshoponExperimentalSoftwareEngineering(ESELAW’14). TrickyBugs.arXivpreprintarXiv:2404.10304(2024).\n[17] ThierryTitcheuChekam,MikePapadakis,YvesLeTraon,andMarkHarman. [40] LechMadeyski,WojciechOrzeszyna,RichardTorkar,andMariuszJozala.2013.\n2017. Anempiricalstudyonmutation,statementandbranchcoveragefault Overcomingtheequivalentmutantproblem:Asystematicliteraturereviewanda\nrevelationthatavoidstheunreliablecleanprogramassumption.InProceedings comparativeexperimentofsecondordermutation.IEEETransactionsonSoftware\nofthe39thInternationalConferenceonSoftwareEngineering,ICSE2017,Buenos Engineering40,1(2013),23–42.\nAires,Argentina,May20-28,2017.597–608. [41] KeMao,MarkHarman,andYueJia.2016.Sapienz:Multi-objectiveAutomated\n[18] YinghaoChen,ZehaoHu,ChenZhi,JunxiaoHan,ShuiguangDeng,andJianwei TestingforAndroidApplications.InInternationalSymposiumonSoftwareTesting\nYin.2024.Chatunitest:AframeworkforLLM-basedtestgeneration.InCompan- andAnalysis(ISSTA2016).94–105.\nionProceedingsofthe32ndACMInternationalConferenceontheFoundationsof\nFSECompanion’25,23–27,2025,Trondheim,Norway Harman,Sengupta\n[42] Meta.2024. IntroducingLlama3.1:Ourmostcapablemodelstodate. https: [53] MaxSchäfer,SarahNadi,AryazEghbali,andFrankTip.2023. Anempirical\n//ai.meta.com/blog/meta-llama-3-1/ evaluationofusinglargelanguagemodelsforautomatedunittestgeneration.\n[43] MilosOjdanic,MikePapadakis,andMarkHarman.2023.Keepingmutationtest IEEETransactionsonSoftwareEngineering(2023).\nsuitesconsistentandrelevantwithlong-standingmutants.InProceedingsofthe [54] DavidSchulerandAndreasZeller.2009.Javalanche:efficientmutationtestingfor\n31stACMJointEuropeanSoftwareEngineeringConferenceandSymposiumonthe Java.In7𝑡ℎjointmeetingoftheEuropeanSoftwareEngineeringConferenceandthe\nFoundationsofSoftwareEngineering.2067–2071. ACMSIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering\n[44] RafaelAPOliveira,UpuleeKanewala,andPauloANardi.2014.Automatedtest (ESEC/FSE2009).297–298.\noracles:Stateoftheart,taxonomies,andtrends.Advancesincomputers95(2014), [55] Richard Speed. 2023. GitHub: 30% of Copilot coding suggestions are ac-\n113–199. cepted. https://www.itpro.com/technology/artificial-intelligence/github-30-of-\n[45] MikePapadakis,YueJia,MarkHarman,andYvesLeTraon.2015.TrivialCom- copilot-coding-suggestions-are-accepted?utm_source=chatgpt.com\npilerEquivalence:ALargeScaleEmpiricalStudyofaSimple,FastandEffective [56] ZhaoTian,HonglinShu,DongWang,XuejieCao,YasutakaKamei,andJunjie\nEquivalentMutantDetectionTechnique.In37𝑡ℎInternationalConferenceon Chen.2024.LargeLanguageModelsforEquivalentMutantDetection:HowFar\nSoftwareEngineering(ICSE2015).Florence,Italy,936–946. AreWe?.InProceedingsofthe33rdACMSIGSOFTInternationalSymposiumon\n[46] MikePapadakis,MarinosKintis,JieZhang,YueJia,YvesLeTraon,andMark SoftwareTestingandAnalysis.1733–1745.\nHarman.2019.MutationTestingAdvances:AnAnalysisandSurvey.Advances [57] FrankTip,JonathanBell,andMaxSchäfer.2024.LLMorpheus:MutationTesting\ninComputers112(2019),275–378. usingLargeLanguageModels.arXivpreprintarXiv:2404.09952(2024).\n[47] JustynaPetke,SaemundurO.Haraldsson,MarkHarman,WilliamB.Langdon, [58] ShreshthTuli,KingaBojarczuk,NatalijaGucevska,MarkHarman,Xiao-YuWang,\nDavidR.White,andJohnR.Woodward.2018.GeneticImprovementofSoftware: andGrahamWright.2023.Simulation-DrivenAutomatedEnd-to-EndTestand\naComprehensiveSurvey.IEEETransactionsonEvolutionaryComputation22,3 OracleInference.In45thIEEE/ACMInternationalConferenceonSoftwareEngi-\n(June2018),415–432. neering:SoftwareEngineeringinPractice,SEIP@ICSE2023,Melbourne,Australia,\n[48] GoranPetrovićandMarkoIvanković.2018.StateofmutationtestingatGoogle.In May14-20,2023.IEEE,122–133.\nProceedingsofthe40thinternationalconferenceonsoftwareengineering:Software [59] LarsvanHijfteandAnaOprescu.2021. Mutantbench:anequivalentmutant\nengineeringinpractice.163–171. problemcomparisonframework.In2021IEEEInternationalConferenceonSoftware\n[49] GoranPetrović,MarkoIvanković,GordonFraser,andRenéJust.2021.Practical Testing,VerificationandValidationWorkshops(ICSTW).IEEE,7–12.\nmutationtestingatscale:AviewfromGoogle. IEEETransactionsonSoftware [60] BoWang,MingdaChen,YoufangLin,MikePapadakis,andJieMZhang.2024.\nEngineering48,10(2021),3900–3912. AnExploratoryStudyonUsingLargeLanguageModelsxforMutationTesting.\n[50] GoranPetrovic,MarkoIvankovic,BobKurtz,PaulAmmann,andRenéJust.2018. arXivpreprintarXiv:2406.09843(2024).\nAnindustrialapplicationofmutationtesting:Lessons,challenges,andresearch [61] JunjieWang,YuchaoHuang,ChunyangChen,ZheLiu,SongWang,andQing\ndirections.In2018IEEEInternationalConferenceonSoftwareTesting,Verification Wang.2023.SoftwareTestingwithLargeLanguageModel:Survey,Landscape,\nandValidationWorkshops(ICSTW).IEEE,47–53. andVision. arXiv:2307.07221.\n[51] CedricRichterandHeikeWehrheim.2022. Learningrealisticmutations:Bug [62] JieZhang,JunjieChen,DanHao,YingfeiXiong,BingXie,LuZhang,andHong\ncreationforneuralbugdetectors.In2022IEEEConferenceonSoftwareTesting, Mei.2014. Search-basedinferenceofpolynomialmetamorphicrelations.In\nVerificationandValidation(ICST).IEEE,162–173. ACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering(ASE’14),\n[52] GabrielRyan,SiddharthaJain,MingyueShang,ShiqiWang,XiaofeiMa,Mu- IvicaCrnkovic,MarshaChechik,andPaulGruenbacher(Eds.).Vasteras,Sweden,\nraliKrishnaRamanathan,andBaishakhiRay.2024.Code-awareprompting:A 701–712.\nstudyofcoverage-guidedtestgenerationinregressionsettingusingLLM.Pro- [63] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,\nceedingsoftheACMonConferenceonFoundationsofSoftwareEngineering1,FSE YonghaoZhuang,ZiLin,ZhuohanLi,DachengLi,EricXing,etal.2023.Judg-\n(2024),951–971. ingLLM-as-a-judgewithMT-benchandchatbotarena. AdvancesinNeural\nInformationProcessingSystems(NeurIPS2023)36(2023),46595–46623.",
    "content_summary": "Mutation-Guided LLM-based Test Generation at Meta\nChristopherFoster AbhishekGulati MarkHarman\nProductComplianceandPrivacy ProductComplianceandPrivacy ProductComplianceandPrivacy\nteam,MetaPlatforms team,MetaPlatforms team,MetaPlatforms\nMenloPark,USA M...",
    "content_length": 65570,
    "created_at": "2025-05-06T21:22:58.517467",
    "updated_at": "2025-05-06T21:22:58.517467",
    "file_path": "unknown_source"
  },
  "doc-773f503eb04d2410b6eee1aa03c536f9": {
    "status": "pending",
    "content": "### Page 1\n\n5\n2\n0\n2\n\nn\na\nJ\n\n2\n2\n\n]\nE\nS\n.\ns\nc\n[\n\n1\nv\n2\n6\n8\n2\n1\n.\n1\n0\n5\n2\n:\nv\ni\nX\nr\na\n\nMutation-Guided LLM-based Test Generation at Meta\nChristopher Foster\nAbhishek Gulati\nProduct Compliance and Privacy\nProduct Compliance and Privacy\nteam, Meta Platforms\nteam, Meta Platforms\nMenlo Park, USA\nMenlo Park, USA\n\nMark Harman\nProduct Compliance and Privacy\nteam, Meta Platforms\nLondon, UK\n\nInna Harper\nDeveloper Infrastructure team, Meta\nPlatforms\nLondon, UK\n\nKe Mao\nWhatsApp team, Meta Platforms\nLondon, UK\n\nJillian Ritchey\nMessenger team, Meta Platforms\nNew York, USA\n\nHervé Robert\nProduct Compliance and Privacy\nteam, Meta Platforms\nMenlo Park, USA\n\nShubho Sengupta\nFAIR, Meta Platforms\nMenlo Park, USA\n\nABSTRACT\nThis paper1 describes Meta’s ACH system for mutation-guided\nLLM-based test generation. ACH generates relatively few mutants\n(aka simulated faults), compared to traditional mutation testing. In-\nstead, it focuses on generating currently undetected faults that are\nspecific to an issue of concern. From these currently uncaught faults,\nACH generates tests that can catch them, thereby ‘killing’ the mu-\ntants and consequently hardening the platform against regressions.\nWe use privacy concerns to illustrate our approach, but ACH can\nharden code against any type of regression. In total, ACH was\napplied to 10,795 Android Kotlin classes in 7 software platforms\ndeployed by Meta, from which it generated 9,095 mutants and\n571 privacy-hardening test cases. ACH also deploys an LLM-based\nequivalent mutant detection agent that achieves a precision of\n0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple pre-\nprocessing). ACH was used by Messenger and WhatsApp test-a-\nthons where engineers accepted 73% of its tests, judging 36% to\nprivacy relevant. We conclude that ACH hardens code against spe-\ncific concerns and that, even when its tests do not directly tackle the\nspecific concern, engineers find them useful for their other benefits.\n\nCCS CONCEPTS\n• Software and its engineering → Software testing and debug-\nging.\n\nKEYWORDS\nUnit Testing, Automated Test Generation, Large Language Models,\nLLMs.\n\n1Author order is alphabetical. The corresponding author is Mark Harman.\n\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0658-5/24/07\nhttps://doi.org/10.1145/3663529.3663839\n\nACM Reference Format:\nChristopher Foster, Abhishek Gulati, Mark Harman, Inna Harper, Ke Mao,\nJillian Ritchey, Hervé Robert, and Shubho Sengupta. 2024. Mutation-Guided\nLLM-based Test Generation at Meta. In Companion Proceedings of the 33rd\nACM International Conference on the Foundations of Software Engineering\n(FSE Companion ’25), 23 – 27, 2025, Trondheim, Norway. ACM, New York,\nNY, USA, 12 pages. https://doi.org/10.1145/3663529.3663839\n\n1 INTRODUCTION\nIn this paper, we report on Meta’s deployment of ACH2, an agen-\ntic LLM-based tool for generating tests to target specific classes\nof faults. The paper focuses on automated privacy hardening: the\nproblem of automatically generating unit tests to reduce the risk\nof future regressions with respect to privacy issues. However, the\napproach can be applied to any issue and is not confined solely\nto tackling privacy issues. The paper reports results from the de-\nployment of ACH at Meta between 28th October 2024 and 31st\nDecember 2024.\n\nAlthough there has been a great deal of recent attention on LLM-\nbased test generation [3, 18, 39, 52, 53], there has been little work\non developing tests for specific classes of fault. Many companies\nhave exposure to specific high impact faults related to important\nissues such as security, integrity, and privacy. The importance of\nsuch issues makes it equally important to have test generation\ntechniques that can target these specific classes of faults.\n\nOrganizations typically collect data about bugs found during de-\nvelopment. This provides a rich source of information with which\nto guide test generation. The challenge, therefore, is to find a way\nto generate tests that target specific issues on the basis of this infor-\nmation. We believe mutation testing holds the key: our key insight\nis to construct mutants that denote faults that are both relevant to\nthe issue of concern and also currently not caught (unkilled) by any\nexisting test case, and to use these as prompts to LLM-based test\ngeneration. This results in an overall agenetic LLM-based workflow,\nin which agents essentially generate problem-specific ‘super bugs’\nand the tests that can catch them.\n\n2ACH (Automated Compliance Hardener) is so-named because it is an automated\ntest generation tool that ‘hardens’ compliance with respect to chosen issues of concern.\n\n### Page 2\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nHarman, Sengupta\n\nAt Meta, we have developed and deployed just such an approach\nand tool, ACH, for automated unit test generation. ACH’s work-\nflow is based on the principle of Assured LLM-based Software\nEngineering [7]. In Assured LLMSE, the goal is not merely to use\nlanguage models to tackle software engineering challenges, but to\nautomatically generate software engineering artifacts that come\nwith assurances. In the case of ACH, the artifacts are tests and they\ncome with the following assurances:\n\n(1) Buildable: The proposed new tests build, so are free from\n\nsyntax errors and missing dependencies;\n\n(2) Valid Regression Tests: The tests pass and do so consis-\n\ntently, so they are non-flaky [26] regression tests;\n\n(3) Hardening: The new tests catch faults that no existing test\n\ncan catch;\n\n(4) Relevant: Many of the new tests are closely coupled to the\n\nissue of concern;\n\n(5) Fashion Following: Most of the new tests respect the cod-\ning style used by the existing tests available for the class\nunder test; they are ‘fashion followers’ [3].\n\nThe first three assurances are boolean in nature; they are un-\nequivocal guarantees made by ACH about the tests it proposes\nto the engineer. The fourth and fifth are more aspirational and\nprobabilistic in nature, reflecting the underlying (LLM) technol-\nogy used to construct tests. The degree to which tests are relevant\nand ‘fashion follow’ the existing tests is an inherently non-boolean\nand subjective attribute. As such, while the boolean criteria are\nprovided as verifiable guarantees that ACH promises to meet, the\ntwo more probabilistic assurances are best assessed through the\nstandard code review process routinely undertaken by engineers.\nFinally, when it finds that a newly-generated test adds coverage,\nACH also measures coverage achieved, including the result in the\nassurances provided to the engineer reviewing the test.\n\nThe newly generated tests need not extend coverage, since they\nmay find faults simply by covering existing lines of code in some\nnew way; the same path with different data, for example. It is\nwell known [17] that mutation testing has this property, allowing\nmutation testing to claim superiority over structural coverage test\nadequacy criteria such as line coverage. As shown in Section 6, our\nfindings further underscore the importance of mutation testing in\n‘going beyond’ purely structural test adequacy criteria.\n\nThe paper focuses on privacy hardening, but we believe that\nour approach will find many applications to software testing, more\ngenerally. It combines existing well-known and widely-studied\napproaches to mutation testing, assured LLMSE, and test generation.\nHowever, this new combination allows it to tackle a wide range of\nother problems. The potential reach and impact of this approach\nderives from the fact that it answers a fundamental question for\nautomated software test generation:\n\nHow can we automatically transform vague, incom-\nplete and even contradictory textual narratives about\nsoftware concerns, into unit tests that guard against\nbugs that, if introduced, may yield field failures that\nmanifest these concerns?\n\nThis fundamental nature makes the approach very widely ap-\nplicable: we can use it in any situation where we have a way to\ncapture, even in the most vague of textual descriptions, concerns\n\nwe have about potential issues our systems will face. Furthermore,\nbecause ACH generates simulated faults as an intermediate stage\nin generating tests, we can use the distributions of simulated fault\nprevalence over the code base as a proxy for the risk exposure\nof individual system components to the issue under test. Finally,\nthrough simulation [2], we can further assess the likely impact (or\nseverity) of such faults, were they to manifest as field failures.\n\nThere are four primary contributions of the paper:\n\nEmpirical results: We report results from the application of\nACH to 7 real world software platforms deployed by Meta. In total,\nACH generated 4,660 candidate mutants (simulated privacy faults)\nthat it ‘believed’ to be potentially killable from 10,795 classes under\ntest. From these candidates, ACH was able to generate an additional\n571 unit tests. Of these 571 tests, 277 would have been discarded\nhad we chosen to focus solely on the line coverage test adequacy\ncriterion, underlining the importance of mutation testing over such\ncoverage.\nDeployment Experience: We report both qualitative and quanti-\ntative outcomes from the development, deployment and evaluation\nof ACH at Meta in 2024. Overall, the tests automatically generated\nby ACH achieved an acceptance rate of 73% from the engineers\nwho reviewed them, with 36% being judged privacy relevant.\nEquivalent Mutant Detection: We present an evaluation of the\nACH equivalent mutant detection agent’s ability to detect equiv-\nalent mutants, which yields precision and recall of 0.79 and 0.47\nrespectively. The figure for precision and recall rises to 0.95 and\n0.96 respectively, when the agent is combined with simple pre-\nprocessing.\nLessons from Industrial Application: We detail the lessons\nlearned, and open problems and research challenges raised by this\napplication of Assured LLMSE to mutation-based test generation.\n\n2 THE ACH SYSTEM\nFigure 1 presents the overall architectural pipeline of the ACH Sys-\ntem. ACH starts with free from text about an issue of concern. This\ntextual input could come from one or more of a variety of sources,\nincluding (but not limited to):\n\n(1) Previous faults found in development;\n(2) User requirements;\n(3) System technical constraints;\n(4) Concerns raised by engineers, managers or organizational\n\nleadership about undesirable system behaviours;\n\n(5) Regulatory requirements set by legislative bodies and com-\n\npliance enforcement organizations.\n\nThis is the sense in which ACH is a ‘compliance hardener’: it\nimproves (‘hardens’) the ability of the deployed regression test in-\nfrastructure to detect regressions that might lead to non-compliance\nwith respect to the issue of concern.\n\nThe results presented in this paper were obtained by using pri-\nvacy hardening concerns from previous faults found in development.\nThe single language model Llama 3.1 70Bn [42] was used in all the\nagents reported on. The prompts used by the three LLM agents\nfrom Figure 1 can be found in Table 1.\n\nOur focus is the development, deployment and evaluation of\nmutation-guided agenetic workflows. We have not yet felt the need\nto extend to more sophisticated prompting, nor to use fine-tuning,\n\n### Page 3\n\nMutation-Guided LLM-based Test Generation at Meta\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nFigure 1: Top level architecture of the principal ACH components. The dotted section denotes a (slightly modified) version\nof the TestGen-LLM tool, on which we previously reported [3]. The workflow preceding this, shown above in the figure, is\nthe additional agenetic workflow for generating candidate faults to drive the generation of tests. Solid rectangles denote\ncomponents that are fully automated but entirely rule-based (and therefore do not use LLMs).\n\nAgent name\nMake a fault\n\nEquivalence detector\n\nMake a test to catch fault\n\nTable 1: The three simple prompts used in the architecture diagram in Figure 1\n\nPrompt Template, in which { · · · } denotes a parameter to the template\nCONTEXT: {context_about_concern} INSTRUCTION: Here is a Kotlin class and a test class with some unit tests for the class under test\n‘‘‘{class_under_test}‘‘‘. ‘‘‘{existing_test_class}‘‘‘. Write a new version of the class under test in which each method is replaced by a new\nversion of that method that contains a typical bug that introduces a privacy violation similar to {diff}. Delimit the mutated part using the\ncomment-pair ‘// MUTANT <START>‘ and ‘// MUTANT <END>‘\nI’m going to show you two slightly different versions of a Kotlin class. Here is the first version of the Kotlin class:‘‘‘class_version1‘‘‘.\nHere is the second version of the Kotlin class:‘‘‘class_version2‘‘‘. INSTRUCTION: If the first version of the class will always do exactly\nthe same thing as the second version of the class, just respond with ‘{yes}‘. However, if the two versions of the class are not equivalent,\nrespond with ‘{no}‘, and give an explanation of how execution of the first version can produce a different behaviour to execution of the\nsecond version.\nWhat follows is two versions of a Kotlin class under test. An original correct class and a mutated version of that class that contains one\nmutant per method, each of which represents a bug. Each bug is delimited by the comment-pair ‘// MUTANT <START>‘ and ‘// MUTANT <END>‘. The\noriginal class and its mutant are followed by a test class that contains unit tests for the original correct class under test. This is the\noriginal version of the class under test:‘‘‘{original_class}‘‘‘. This is the mutated version of the class under test:‘‘‘{mutated_class}‘‘‘.\nHere is the existing test class:‘‘‘{existing_test_class}‘‘‘. Write an extended version of the test class that contains extra test cases that\nwill fail on the mutant version of the class, but would pass on the correct version.\n\nnor to exploit language model ensembles [3], all of which would\nimprove on the results we report. Therefore, our results denote\nonly a baseline against which to measure futures developments.\nNevertheless, despite these limitations, we believe the results clearly\nhighlight the potential for significant advances in test generation; its\napplicability and effectiveness. We would be interested and excited\nto collaborate with the wider research community, and hope this\npaper stimulates further work in this area.\n\nIn the ACH workflow depicted in Figure 1, the issue summary\ngenerates prompts that stimulate another LLM-based agent to gen-\nerate faults; walking the code base, using existing tests, classes\nunder test, and the summary as prompt ingredients. The prompts\ninstruct the LLM to generate simulated faults (aka mutants [35]).\nThere is no guarantee that these mutants will change the code under\ntest, because it may not be relevant to the issue being hardened.\n\nHowever, even when the fault generator does change the syntax,\nit may not change the semantics; the mutants could be equivalent\n\nmutants [27]. To tackle this equivalent mutant problem, ACH uses\na further agent, the Equivalence Detector agent. This Equivalence\nDetector agent uses the LLM-as-judge approach [24, 63]. The judge\nis instructed to determine whether the mutant is equivalent to\nthe original class under test. Although the underlying problem of\nmutant equivalence is undecidable [35], we found that this approach\nwas surprisingly effective due to the properties of the mutants so-\ngenerated (Section 5 contains details).\n\nThe faults generated by the initial phase of the workflow are\nused to generate prompts for test generation. The test generation\nphase shown in a dotted box is a slightly modified version of the\npreviously published TestGen-LLM tool [3].\n\n3 RESULTS FROM META’S ACH DEPLOYMENT\nWe applied ACH to the 7 Meta platforms Aloha, Facebook Feed,\nInstagram, Messenger, Oculus, Wearables, and WhatsApp. Facebook\n\nPrivacy faultPassesYesNonoBuildsBuildsYesPasses on originalPasses on   faultFaultNew testsNoNoYesYesLLMMake a faultLLMMake a testto catch faultCurrent Issues from CIOnwardcode reviewin CIYesNoExisting test classClass under testCurrent code repositoryLLMEquivalencedetectorEquivalent?yesNoDotted section is a version of the TestGen-LLM workflowAuto-generate diff summary and test planRemove syntactically identical faultsDiscarded faultsDiscarded new testsIssue summary\n\n### Page 4\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nHarman, Sengupta\n\nFeed and Instagram are social media platforms. WhatsApp and\nMessenger are messaging platforms. Oculus is a set of virtual reality\nheadsets. Wearables are platforms for augmented reality glasses and\nwrist controllers. We also included ‘cross-app’ software products\nthat provide features affecting more than one of these platforms.\nTable 2 presents top level summary statistics for the application\nof ACH to these platforms. Once a mutant is found that builds and\npasses, the workflow terminates3 for the current class under test.\nThe number of candidate mutants reported in Table 2 is therefore\nbounded above by the number of classes under test.\n\nBy contrast with traditional rule-based mutation testing tools [19,\n33, 37, 54], ACH generates relatively few, highly specific mutants,\nby design. Furthermore, the generated mutants have a higher prob-\nability of relevance to the issue of concern than can be achieved by\nrule-based approaches.\n\nAs can be seen from Table 2, ACH generates 9,095 mutants that\nbuild and pass from 10,795 classes under test. Although the lan-\nguage model approach generates fewer and more specific mutants\nthan rule-based approaches, we found that it also generates more\nequivalent mutants. For example, as Table 2 reveals, 25% of the mu-\ntants generated are trivially syntactically equivalent. This contrasts\nwith overall equivalent mutant generation rates that are typically\naround 10% to 15% for rule-based approaches [27, 45]. Fortunately,\nas revealed in Section 5, the equivalent mutant problem is relatively\nunimportant for our use case.\n\nOf the 9,095 mutants that build and pass, 4,660 (51%) were deemed\nto be non-equivalent by the workflow, and thus become the subject\nof each of the prompts used for test generation. The test generation\nphase uses similar prompts to those reported previously [3], but\nbased on covering the mutant rather than covering uncovered lines.\n\n4 ENGINEERS’ EVALUATION OF ACH\nWe followed the tried-and-tested formula [3] for the deployment\nof a new software testing technology at Meta, starting with initial\ntrials and moving onto to test-a-thons led by engineers, thereby\nevaluating initial deployment.\n\n4.1 Initial Trial\nAt Meta, a code change submitted to the Continuous Integration (CI)\nsystem is called a ‘diff’ (short for differential). In order to get initial\nfeedback from engineers on the tests generated by the first version\nof ACH, we used it to generate 30 new test cases, submitting each as\na separate diff for review. The generated tests were recommended\nto engineers in the same way as any other code modification, such\nas those created by human engineers. That is, ACH submits tests,\nas diffs, through the normal review process. The ACH diff summary\nexplains the kind of fault caught by the test case giving, as a specific\nexample, the mutant that ACH has already determined to be killed\nby the test case.\n\nWe wanted to obtain broad coverage of Meta’s platforms, so\nincluded messaging apps like WhatsApp and Messenger, traditional\nsocial media platforms such as Facebook Feed, as well as hardware\n\n3This decision was motivated by the use case: we first want to establish whether a test\nis relevant. When we find a test that is relevant, we can choose to focus in on the code\nit tests, hoping that adjacent code will also have a higher likelihood of relevance.\n\nFigure 2: Likert scale instructions given to code reviewers to\nscore test cases according to their privacy relevance.\n\ntechnologies such as Oculus and Wearable Computing, and also\ncode that spanned over multiple apps.\n\nTable 3 presents details of these 30 diffs. As can be seen, the\noverall acceptance rate is 90% accepted of those submitted (27 of 30),\nand 93% of all diffs reviewed (27 of 29). Initial results for acceptance\nrate were deemed to be highly encouraging, so we proceeded to\nbuild out the Minimal Viable Product (MVP) and use it in the test-\na-thons reported in Section 4.2.\n\nThe primary feedback from developers who reviewed these 30\n\ndiffs was as follows. Engineers reported that\n\n(1) Many of the tests added coverage, which they computed\nthemselves manually. They asked that coverage information\nbe automatically computed and reported in the diff sum-\nmaries, as well as the faults found.\n\n(2) Some of the tests were relevant to privacy, and could be\nuseful in hardening against future privacy regressions. They\nalso felt that having the specific example faults was very\nhelpful in understanding the behavior of the tests.\n\n(3) Even for those which were not clearly related to privacy, the\ntests seemed to be adding value by tackling corner cases or\nadding coverage.\n\n4.2 Experience from Privacy Test-a-thons\nIn the week of 9th December 2024, we conducted two test-a-thons,\nfocusing on the application of ACH to Meta’s two messaging plat-\nforms: WhatsApp and Messenger.\n\nAs with the initial trial, test cases were submitted as diffs into\nthe normal continuous integration review process. However, the\ndiff summary additionally claimed additional coverage, for those\ntests that did add coverage as well as finding currently uncatchable\nfaults. The engineers participating in the test-a-thons reviewed the\ndiffs for usefulness in the normal way they would any other diff,\nultimately determining whether the diff is accepted, and thus lands\ninto production.\n\nIn order to evaluate the privacy relevance of each test, we ad-\nditionally asked the software engineers to give each a score on\na Likert scale [32]. Figure 2 is a screen capture of the exact in-\nstructions given to the engineers regarding this privacy relevance\nscoring procedure.\nProcedure for Messenger two-phase test-a-thon: The Messen-\nger test-a-thon was conducted in two phases. The same 6 reviewers\nwere used for both the first and the second phase. All Messenger\nreviewers had strong expertise in testing, and (at least a) basic\nexperience with privacy engineering.\n\nIn the first phase, 50 generated tests were selected from a ran-\ndomly selected pool of 60 tests. All 60 were prescreened manually\n\n### Page 5\n\nMutation-Guided LLM-based Test Generation at Meta\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nTable 2: Meta platforms used to evaluate ACH and the numbers of simulated privacy faults ‘believed’ non-equivalent by the\nequivalence detector agent from Figure 1. Percentages in the final four columns are distributions of ACH’s equivalence ‘belief’\nover mutants that build and pass, while those in the fourth column report the percentage of all mutants that build and pass.\n\nPlatform\n\nFacebook Feed\nMessenger\nInstagram\nAloha\nWearables\nCross-app\nOculus\nWhatsApp\n\nclasses\nunder test\n\nNumber of Number of Number of\nmutants\nmutants\ngenerated\ngenerated\nthat build\nin total\nand pass\n252 (23%)\n2,922 (32%)\n1,381 (27%)\n144 (25%)\n2,468 (29%)\n562 (31%)\n279 (28%)\n1,087 (26%)\n\n346\n3,339\n1,691\n175\n2,841\n805\n325\n1,273\n\n1,097\n9,185\n5,199\n576\n8,592\n1,819\n997\n4,212\n\nNumber of mutants that build and pass and ...\n\nare\nsyntactically\nidentical\n\n50 (20%)\n742 (25%)\n277 (20%)\n32 (22%)\n621 (25%)\n146 (26%)\n96 (34%)\n282 (26%)\n\nthe agent\nbelieves\nto be\nequivalent\n19 (7.5%)\n384 (13%)\n154 (11%)\n16 (11%)\n207 (8%)\n64 (11%)\n17 (6%)\n155 (14%)\n\nfor which\nthe agent\ngives no\nanswer\n19 (7.5%)\n401 (14%)\n208 (15%)\n6 (4%)\n326 (13%)\n82 (15%)\n16 (6%)\n115 (11%)\n\nthe agent\nbelieves\nto be non-\nequivalent\n164 (65%)\n1,395 (48%)\n742 (54%)\n90 (63%)\n1,314 (53%)\n270 (48%)\n150 (54%)\n535 (49%)\n\nTotals\n\n10,795\n\n31,677\n\n9,095 (29%)\n\n2,246 (25%)\n\n1,016 (11%)\n\n1,173 (13%)\n\n4,660 (51%)\n\nTable 3: Results from initial trial ACH deployment on 6 plat-\nforms and cross-app products to gauge developers’ reactions,\ngain feedback and guide development.\n\nStatus after human review is ...\n\nPlatform\n\nFB Feed\nMessenger\nAloha\nWearables\nCross-app\nOculus\nWhatsApp\n\nTotals\n\nNumber\nof tests Accepted Accepted\nwith\n‘as is’ by\nsimple\nthe\nchanges\nengineer\n0\n4\n3\n8\n0\n2\n0\n1\n0\n2\n0\n3\n1\n3\n\n4\n12\n2\n3\n2\n3\n4\n\n30\n\n23\n\n4\n\nRejected\nby the\nengineer\n\nNot\nreviewed\n\n0\n1\n0\n1\n0\n0\n0\n\n2\n\n0\n0\n0\n1\n0\n0\n0\n\n1\n\nand, as a result, 10 were excluded because they concerned only\nfaults relating to Null Pointer Exceptions, two of which improved\ncoverage, and eight of which did not. These were excluded because\nwe initially (wrongly, as it turned out) thought that such exception-\ncatching tests would be rejected by engineers due to being irrelevant\nto privacy. However, the pre-screening approach was abandoned\nbased on experience from this first phase: Engineers proved ready\nto accept tests, even when they were not directly relevant, because\nthey perceived other benefits. Therefore, in the second phase, a\nfurther 50 tests were selected at random, without pre-screening.\nProcedure for the WhatsApp test-a-thon: In the WhatsApp\ntest-a-thon, 72 tests4 were selected randomly from a pool of 120\navailable and allocated to 6 engineers to review. No pre-screening\nwas performed. All 6 engineers had a background in both privacy\nengineering and software testing and, as such, were well-placed\nwith relevant expertise and highly calibrated in their expectations\nabout privacy concerns.\n\n4The decision to use 72 tests was based on the initial request from the 6 engineers that\nthey could reasonably review 12 tests; this number being deemed large enough for\ncalibration, yet small enough to avoid reviewer overload.\n\nHaving completed the reviews of the initial 72 tests allocated,\nseveral of the engineers requested additional tests to review, hav-\ning found the experience sufficiently rewarding. Additional tests\nwere therefore allocated at random from the remaining pool of 48\navailable. Therefore, in total, 91 WhatsApp tests were reviewed for\nusefulness, and 90 were reviewed for relevance.\nResults from the test-a-thons: Table 4 presents summary sta-\ntistics for the two test-a-thons. The upper table gives the number\nof tests reviewed for usefulness and for privacy relevance and, of\nthese, the number that were accepted and rejected for usefulness,\nand scored for relevance. The lower table gives the proportions of\ntests reviewed for usefulness that were accepted and rejected, and\nthe proportion of those reviewed for relevance that fall into each\nof the five categories of the Likert scale on which relevance was\nassessed by the software engineers. Percentages are rounded to the\nnearest whole number percentage point, and so may not quite total\n100% due to rounding.\n\nAs Table 4 reveals, the overall acceptance rate was 73%. This is\nvery similar to acceptance rates from previous test-a-thons that\nfocused exclusively on elevating coverage [3]. Overall, about one\nthird of the tests deemed definitely irrelevant to privacy. This makes\nit all the more interesting that the rejection rate is much lower than\nthis, at 27%. Indeed, only approximately 36% were deemed to be\neither possibly or definitely related to privacy. We therefore observe\nthat the engineers are very willing to accept tests which may not\nbe relevant to their current concern, when they are found useful\nfor other reasons.\n\nIn looking at the individual comments left by the engineers on\neach of the 191 tests that were reviewed for usefulness, we observe,\nmore anecdotally, that the engineers were likely to accept tests for\ntwo primary reasons:\n\n(1) They add line or branch coverage of non-trivial code\n(2) They covered a tricky corner case, such as handling special\n\nvalues, even when this failed to add coverage\n\nTests that were rejected tended not to add coverage, or to add\ncoverage only of trivial code, such as one-line behavioral functions.\nTests were also rejected if they were written in a style that was\n\n### Page 6\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nHarman, Sengupta\n\nTable 4: Results from WhatsApp and Messenger Test-a-thons, where ACH was deployed and evaluated in December 2024.\n\nTest-a-thon\nWhatsApp\nMessenger Phase 1 (pre-screened)\nMessenger Phase 2 (not pre-screened)\nOverall Total\n\ntotal number\nof tests\nreviewed\nfor usefulness\n91\n50\n50\n191\n\ntotal number\nof tests\nthat were ...\n\naccepted\n50\n47\n43\n140\n\nrejected\n41\n3\n7\n51\n\ntotal number\nof tests\nreviewed\nfor relevance\n90\n44\n41\n175\n\n5\n6\n5\n4\n15\n\nProportions of tests in each category for usefulness and relevance :-\n\nprivacy relevance\nscored at level ...\n2\n24\n4\n7\n35\n\n4\n29\n8\n11\n48\n\n3\n8\n5\n0\n13\n\n1\n23\n22\n19\n64\n\nOver all test-a-thons\n\nWhatsApp\nMessenger Phases 1 and 2\nMessenger Phase 1 alone\nMessenger Phase 2 alone\n\n73%\n\n56%\n90%\n94%\n86%\n\n27%\n\n44%\n10%\n6%\n14%\n\n9% 27%\n\n7% 20% 37%\n\n7% 32%\n11% 22%\n11% 18% 11%\n10% 27%\n\n9% 26% 26%\n6% 13% 48%\n9% 50%\n0% 17% 46%\n\ndeemed to be unsuitable, such as using a deprecated API, something\nthat was also observed in previous work [3].\n\nIn terms of the relevance of the tests, we believe that the find-\ning that 36% are deemed relevant to privacy is a positive overall\noutcome. This is because we devoted so little of the overall process\nto weeding out tests that could be automatically determined to be\nirrelevant. As such, the result for relevance can be considered a\nbaseline for comparison with future work. We believe that with\nadditional static analysis, and further LLM-as-judge agents in the\noverall workflow, it could be considerably improved.\n\nWe also noticed that the comments about relevance for tests\nscored 4 and 2 were often quite similar, indicating uncertainty,\nand the belief that the test may be relevant to privacy, but the\nengineer was not certain. Therefore, an upper bound on those\npotentially relevant to privacy is approximately two thirds of those\ntests assessed. Given the 73% acceptance rate, we conclude that\nACH adds privacy hardening in about one third of the cases, and\ndoes not cause unnecessary developer friction in considering the\nremaining cases.\n\nThere is an interesting difference in the relevance score between\nthe two phases of the Messenger test-a-thon results reported in\nTable 4. In the first phase, the engineers stated that they were\nunsure in 10% of the cases, but this percentage dropped to zero\nin the second phase. This apparent growing scoring ‘confidence’\nindicates a potential ‘learning effect’, which has been seen in similar\nempirical studies of software engineers during multiphase trials\n[21].\n\nFinally, looking at the differences between the acceptance rates\nand relevance assessments for WhatsApp and Messenger, we also\nsee interesting differences. WhatsApp engineers rated the tests\nto be at least as relevant to privacy (39% for WhatsApp vs. 33%\nfor Messenger), yet accepted fewer tests (56% WhatsApp vs. 89%\nfor messenger). All tests that were acceptable for usefulness were\nlanded into production. We believe that the different acceptance\nrates may simply denote different cultures between different teams;\ndeciding to land a test into production is an inherently subjective\njudgment and may be influenced by team culture.\n\n5 THE EQUIVALENT MUTANT PROBLEM\nAll approaches to mutation testing need to tackle the problem of\nequivalent mutants [35]: the mutant may be syntactically different\nto the original program, but we cannot guarantee it will be se-\nmantically different, because the underlying program equivalence\nproblem is undecidable [27, 59].\n\nThere are a number of techniques in the literature that can weed\nout some of the equivalent mutants [40]. However, the undecid-\nability of the problem means that some equivalent mutants will\ninevitably remain, so engineers and automated test generators may\nwaste time trying to kill (unkillable) equivalent mutants.\n\n5.1 Equivalent Mutants Have no Impact on ACH\nFor the application of mutation-guided test generation, the equiva-\nlent mutant problem has no direct impact on engineers. Our work-\nflow requires only that engineers review test cases, not mutants.\nThe only scenario in which an engineer might consider looking at a\nmutant, would be to see an example of the kind of faults that can be\ncaught by the test case they are reviewing. By construction, such\nmutants are non-equivalent, so the engineer will never see an equiv-\nalent mutant. This relegates the equivalent mutant problem to a\nrelatively subordinate position in our overall use case for mutation\ntesting.\n\nNevertheless, it would be inefficient to generate many equivalent\nmutants, because ACH would waste computational resources trying\nto kill the unkillable. We therefore incorporate, into the agentic\nworkflow, an LLM-based agent that checks for mutant equivalence.\nThe remainder of this section reports on the evaluation of the\neffectiveness of this agent.\n\n5.2 Detecting Equivalent Mutants\nTo evaluate the performance of the equivalence detector agent from\nFigure 1, we performed a manual study on a random selection of\nmutants drawn from the four platforms with the most mutants\navailable. The purpose of this manual analysis is to answer the\nresearch question:\n\nHow good is the Equivalence Detector Agent?\n\n### Page 7\n\nMutation-Guided LLM-based Test Generation at Meta\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nWe must manually analyze mutants to answer this research ques-\ntion, because it requires a ground truth for equivalence. However, it\nis important to underline that it is never necessary for a practicing\nsoftware engineer to manually evaluate any mutant.\n\nFor each of the four platforms, we attempted to sample 100\nmutants that would still build and pass on the original code. The\nactual number manually analyzed for each sample was slightly\ndifferent to the intended 100, because some code may have changed\nsince the mutants were constructed. This is the Mutant Relevance\nProblem [43]: previously generated mutants may no longer be\nrelevant when the code changes after they were constructed.\n\nMutant relevance does not impact the deployment of ACH, since\nACH generates mutants on the fly, discarding them once tests have\nbeen generated from them. However, it did have a minor impact\non our mutant sampling to evaluate the research question: Since\nwe cannot be sure mutants in changed code remain valid at man-\nual inspection time, we discarded them from the human analysis\nsample.\n\nIn our evaluation of equivalence detection, we are concerned only\nwith so-called ‘weak’ mutation testing, not ‘strong’ mutation testing\n[35]. That is, because ACH is a unit test generation technology, we\ndo not need to consider failed error propagation [9], which would be\nrequired for strong mutation testing. Failed error propagation is the\nsituation in which mutant execution changes the local computation\nstate, yet this change is always masked along every path to an\nobservable output. In weak mutation testing, the mutant is non-\nequivalent if the local state is changed, irrespective of the whether\nthe change propagates. Manually detecting failed error propagation\nis a highly labor-intensive process, so we are fortunate that it is not\nrequired.\n\nTable 5 describes the types of mutant, based on our manual anal-\nysis. In this analysis, the terms ‘deletion’ and ‘injection’ each refer\nto semantic changes where the sole change is, respectively, the\naddition of values or the removal of values. The term ‘Injection\nand deletion’ refers to cases where the semantics are changed by\nan independent combination of addition and removal. The cate-\ngory ‘other’ covers cases where the change is more nuanced. For\nexample replacing one variable name with another, which could be\ncounted as a deletion and an injection, but where the two are not\nindependent.\n\nIn Table 5, the column ‘equivalence is not obvious’ counts the\nnumber of cases where the human assessor had to think noticeably\nlonger about whether the mutant was equivalent. For all other\ncases, the decision was obvious. As Table 5 reveals, the decision is\n‘obvious’ for a surprisingly large number of mutants.\n\nThe category ‘Misleading comment’ is perhaps the most interest-\ning category. These are cases where the mutant is clearly equivalent,\nbecause all the mutant generation agent has done is to insert com-\nments; no executable code has been added. An example of such a\nmisleading injected comment is: “// Introduce a bug by not\nchecking if the user is inactivated before resetting\ntheir custom reactions”. Since language models are text-based\npredictive technologies that currently do not specifically distin-\nguish executable code from comments, such misleading comments\nare clearly problematic. They can easily mislead the equivalence\ndetector agent into predicting that a mutant is not equivalent, when\nit clearly is equivalent.\n\nThe reason that this category is so interesting, is that ACH could\nsimply strip out any comments added when constructing a mutant\nbefore submitting it to the equivalence detector agent. This simple\npre-processing step would render all cases of misleading comments\nsyntactically identical. They would then be automatically discarded\nwithout even needing to consult the Equivalence Detector Agent.\nBased on the evaluation reported here, we decided to incorporate\nthis pre-processor into ACH’s workflow.\n\n5.3 Equivalence Detector Precision and Recall\nTable 6 reports the overall precision and recall of the equivalence\ndetector agent. The detailed results for each platform are shown\nin Table 7. As Table 6 reveals, when we consider ‘unsure’ as the\nsame as ‘equivalent’, the precision is good, at 0.79 over all platforms\nstudied. However, recall is relatively low, at 0.47. If we wanted even\ngreater precision we could treat ‘unsure’ as ‘non-equivalent’, since\nthis gives precision of 0.97 (and a recall of 0.44).\n\nBased on this precision and recall, we can have high confidence\nthat, when the detector determines a mutant to be equivalent, it is\nvery likely to be correct. However, it weeds out only approximately\nhalf of the equivalent mutants. High precision means ACH will not\ndiscard many non-equivalent mutants. Not losing mutants means\nnot losing the tests that ACH might generate from them. By contrast,\nhigher recall would merely save some computational resource.\n\nAs such, high precision is generally more valuable than high\nrecall for our use case, because we are prepared to spend computa-\ntional resources to automatically generate good unit tests. However,\nthe lower the recall, the more often ACH will inefficiently seek to\n‘kill the unkillable’, so we still want the highest recall achievable\nwithout reducing precision.\n\nAs Table 2 shows, approximately 25% of all mutants are trivially\nequivalent, being syntactically identical to the original. Further-\nmore, Table 5 reveals that 61% of all equivalent mutants are almost\ntrivially equivalent, because they contain only a single ‘misleading’\ncomment. Syntactically identical mutants are removed by a simple\nlexical comparison, while a simple pre-processing transformation\nto remove comments would remove the ‘misleading comments’\ncategory. Therefore, we can combine the LLM-based detector with\na simple rule based pre-processor to yield an overall precision and\nrecall of 0.95 and 0.96 respectively (See Table 6).\n\nThese figures for precision and recall are surprisingly high, given\nthat the underlying problem is undecidable. However, we cannot\nclaim that these surprisingly good results arise because the mutant\nequivalence detector agent is excellent at determining program\nequivalence. Rather, they are more reflection of the kind of mutants\nthe detector needs to judge for equivalence. That is, the mutant\ngeneration agent tends to make changes that either obviously intro-\nduce semantic changes, or are obviously equivalent (such as those\nthat only change comments).\n\n6 THE IMPORTANCE OF MUTATION TESTING\nIt is well known in the literature on testing, both theoretically\nand empirically, that mutation adequacy criteria outperform tradi-\ntional structural criteria, such as line coverage and branch coverage\n[17, 46]. In this section, we present results that further underline\n\n### Page 8\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nHarman, Sengupta\n\nTable 5: Number of mutants of different semantic types generated. We used human analysis to determine ground truth\nequivalence, and the different types of semantic transformation used to generate a mutants.\n\nPlatform\n\nMessenger\nWearables\nWhatsApp\nInstagram\n\nTotal\nmutants\nanalyzed\n91\n101\n99\n90\n\nGround\ntruth\nequivalent\n38 (42%)\n32 (32%)\n30 (30%)\n37 (41%)\n\nmutant equivalence\nLLM agent\nclaims\nequivalence\n27 (30%)\n11 (12%)\n16 (16%)\n12 (13%)\n\nEquivalence\nis not\nobvious\n2 (2%)\n3 (3%)\n3 (3%)\n2 (2%)\n\nDeletion\n\nsemantic type of mutation transformation\nInjection Deletion Misleading\ncomment\n\nand\ninjection\n19 (21%)\n20 (20%)\n16 (16%)\n11 (12%)\n\n11 (12%)\n24 (24%)\n26 (26%)\n32 (36%)\n\nOther\n\n6 (7%)\n7 (7%)\n2 (2%)\n6 (7%)\n\n23 (25%)\n26 (26%)\n29 (29%)\n27 (30%)\n\n32 (35%)\n24 (24%)\n26 (26%)\n14 (16%)\n\nOverall Totals\n\n381\n\n137 (36%)\n\n66 (17%)\n\n10 (2.6%)\n\n105 (28%)\n\n96 (25%)\n\n66 (17%)\n\n93 (24%)\n\n21 (6%)\n\nTable 6: Precision & recall for the equivalence detector agent\nwhen unsure is simply not counted\n\nTP\n56\n\nTP\n65\n\nFP\n2\n\nTN FN\n72\n205\n\nPrecision\n0.97\n\nRecall\n0.44\n\nwith unsure counted as equivalent\n\nFP\n17\n\nTN FN\n72\n205\n\nPrecision\n0.79\n\nRecall\n0.47\n\nwith identical code counted as equivalent\nRecall\nTP\n0.69\n161\n\nPrecision\n0.90\n\nTN FN\n72\n205\n\nFP\n17\n\nwith the stripping of all added comments\nRecall\nTP\n0.96\n183\n\nPrecision\n0.95\n\nTN FN\n8\n251\n\nFP\n9\n\nthese previous research findings with direct experience from the\ndeployment of mutation testing on large scale industrial systems.\nTable 8 reports line coverage results for the tests that kill non-\nequivalent mutants. For Messenger, Instagram, Wearables and\nWhatsApp the ‘likely actual’ number is based on the ground truth\nequivalent proportion for these platforms in Table 5. For the other\nplatforms, it is based on the overall average ground truth proportion\nover all four platforms. The final three columns show the results\nfor TestGen-LLM [3] generated tests, as proportions for compari-\nson with the corresponding proportions for ACH generated tests.\nTestGen-LLM does not target specific faults, but merely attempts\nto generate tests that will acquire extra coverage.\n\nAs Table 8 shows, a large proportion (49%) of test cases that\nuniquely additionally kill a mutant, do not also add line coverage.\nClearly, these tests have value, since they catch faults that would\notherwise go undetected. However, if we were to judge test cases\nsolely on the basis of the coverage they add, then such valuable test\ncases would be wrongly discarded.\n\nThe results in Table 8 also reveal that, although TestGen-LLM\ngenerates tests for a higher proportion of classes compared to\nACH (32% vs. 5.3%), it nevertheless, succeeds in killing a far smaller\nproportion of mutants (2.4% vs 15%). This is because ACH specifi-\ncally targets the mutants, whereas TestGen-LLM does not.\n\nFrom these results, we conclude that targeting mutants can also\nelevate coverage, but targeting coverage will be inadequate to kill\nmutants. Although this is already known from the research liter-\nature [17, 46], that literature is based on laboratory controlled ex-\nperiments with non-industrial systems. Our findings thus augment\n\nthe existing literature and add weight to claims for the importance\nof mutation testing, based on industrial practice and experience.\n\nIt is also worth noting that 70% of the mutants left unkilled\nby TestGen-LLM reside in classes that do not have any TestGen-\nLLM test, let alone one that kills the mutant. Combined with the\nhigh proportion (51%) of ACH tests that do also raise coverage, we\nconclude that Mutation-Guided LLM-based Test Generation has\nattractive side benefits on coverage.\n\nSpecifically, we may be able to adapt our ACH approach to target\ncoverage using mutants: Suppose we generate many mutants, of\nall different kinds, and at all different levels of abstraction, for\na given method under test. We can then use each as a prompt\nto an LLM. In this way, mutants play a role similar to Retrieval\nAugmented Generation (RAG). Given our results and the fact that\nRAG is known to improve LLM performance on other Software\nEngineering tasks [22], we believe mutation-as-RAG for coverage\nis likely to prove attractive for coverage as well as for targeting\nspecific faults.\n\n7 RELATED WORK\nGiven the strong empirical evidence for the predictability of code\n[10, 23, 28], it is unsurprising that predictive language models have\nproved effective at generating usable code. As a result, LLMs now\nplay a code-generation role in a significant number of applications\nacross the spectrum of software engineering activity [22].\n\nSoftware engineers’ acceptance rates for LLM-generated code\nhave been widely studied. For example, researchers at Google fo-\ncussed on factors that may influence trust in AI-powered code\ncompletions [13], while it was recently reported that Copilot had\nan acceptance rate of approximately 30% [55].\n\nHowever, such studies concern the code completion use case,\nwhich does not come with any assurances (the code completions\nmay not even compile). By contrast, ACH uses Assured LLM-Based\nSoftware Engineering (Assured LLMSE) [7]. That is, ACH provides\nassurances about the semantics and usefulness of the tests it pro-\nposes. Its proposals are also whole compilable code units (not merely\ncompletion fragments), and it is deployed on-demand (to gener-\nate tests targeting classes of faults of particular interest), rather\nthan opportunistically (to suggest code completions). The fact that\nACH provides these assurances, and its different deployment route,\nmean that we can expect a higher overall acceptance rate from\nACH, than we would for code completion technologies.\n\n### Page 9\n\nMutation-Guided LLM-based Test Generation at Meta\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nTable 7: Precision (Prec.) and Recall achieved by the equivalence detector agent for the four platforms with most mutants\n\nMessenger Mutants\n\nWearables Mutants\n\nWhatsApp Mutants\n\nInstagram Mutants\n\nFP\n2\n\nFP\n3\n\nFP\n3\n\nTN\n50\n\nTN\n50\n\nTN\n50\n\nFN\n19\n\nFN\n19\n\nFN\n19\n\nWhen the agent is ‘unsure’, this is simply not counted in determining precision and recall\n\nPrec.\n0.89\n\nRecall\n0.47\n\nTP\n11\n\nFP\n0\n\nTN\n55\n\nFN\n18\n\nPrec.\n1.0\n\nRecall\n0.38\n\nTP\n16\n\nFP\n0\n\nTN\n51\n\nFN\n11\n\nPrec.\n1.0\n\nRecall\n0.59\n\nWhen the agent is ‘unsure’, this is counted as if it had determined the mutant to be equivalent\n\nPrec.\n0.86\n\nRecall\n0.50\n\nTP\n14\n\nFP\n1\n\nTN\n55\n\nFN\n18\n\nPrec.\n0.93\n\nRecall\n0.50\n\nTP\n19\n\nFP\n9\n\nTN\n51\n\nFN\n11\n\nPrec.\n0.68\n\nRecall\n0.63\n\nTP\n12\n\nTP\n13\n\nFP\n0\n\nFP\n4\n\nWhen we include mutants that are syntactically identical in the numbers counted as determined to be equivalent\nFP\n4\n\nRecall\n0.70\n\nRecall\n0.58\n\nRecall\n0.81\n\nPrec.\n0.98\n\nPrec.\n0.84\n\nTN\n51\n\nTN\n55\n\nFN\n11\n\nFN\n18\n\nTP\n31\n\nTP\n42\n\nTP\n47\n\nFP\n1\n\nFP\n9\n\nPrec.\n0.93\n\nTN\n49\n\nTN\n49\n\nTN\n49\n\nFN\n24\n\nFN\n24\n\nFN\n24\n\nPrec.\n1.0\n\nRecall\n0.33\n\nPrec.\n0.76\n\nRecall\n0.35\n\nPrec.\n0.86\n\nRecall\n0.56\n\nWhen we additionally include, as determined to be equivalent, those mutants that are syntactically identical after stripping any comments added by the generation agent\n\nFP\n3\n\nTN\n67\n\nFN\n2\n\nPrec.\n0.93\n\nRecall\n0.95\n\nTP\n42\n\nFP\n1\n\nTN\n69\n\nFN\n4\n\nPrec.\n0.93\n\nRecall\n0.91\n\nTP\n47\n\nFP\n1\n\nTN\n66\n\nFN\n0\n\nPrec.\n0.99\n\nRecall\n1.0\n\nTP\n53\n\nFP\n4\n\nTN\n49\n\nFN\n2\n\nPrec.\n0.93\n\nRecall\n0.96\n\nTP\n17\n\nTP\n19\n\nTP\n41\n\nTP\n41\n\nPlatform\n\nNumber of\nmutant-\nkilling\ntests\n\nNumber of\n(believed;\nlikely actual)\nnon-equivalent\nmutants\n\nTable 8: The numbers of tests that add coverage as well as detecting mutated faults. This gives an upper bound on the number\nof faults we might miss by merely targeting new coverage alone. Approximately half of all tests generated, although finding\nfaults missed by all existing tests, do not add line coverage. Had we used coverage as our sole test adequacy criterion, the\nplatforms would thus have continued to be vulnerable to regressions denoted by up to approximately half of the faults.\n%age\n%age\nTestGen-\nunkilled\nLLM kill mutants\nwith no\n(believed; TestGen-\nLLM test\n71%\n71%\n74%\n76%\n70%\n75%\n70%\n55%\n70%\n\n%age\nNumber\nClasses\n(%age)\nof tests\nwith a\nthat do TestGen-\nLLM test\nnot add\ncoverage\n9 (60%)\n84 (43%)\n77 (55%)\n4 (44%)\n32 (71%)\n20 (57%)\n3 (21%)\n48 (36%)\n277 (49%)\n\n%age\nof classes\nwith a\nmutant-\nkilling\ntest\n4.3%\n5.9%\n7.2%\n5.1%\n1.6%\n4.3%\n4.3%\n10.6%\n5.3%\n\nFacebook Feed\nMessenger\nInstagram\nAloha\nWearables\nCross-app\nOculus\nWhatsApp\nTotals\n\nlikely actual)\n3.0%; 3.7%\n2.0%; 2.4%\n2.9%; 3.1%\n1.0%; 1.2%\n1.9%; 2.5%\n2.8% 2.7%\n4.9%; 6.1%\n3.5%; 4.2%\n2.0%; 2.4%\n\n164; 133\n1,395; 1,155\n742; 687\n90; 74\n1,314; 1,007\n270; 275\n150; 121\n535; 445\n4,660; 3,897\n\n9%; 11%\n14%; 17%\n16%; 18%\n10%; 12%\n3%; 4%\n13%; 13%\n9%; 12%\n25%; 30%\n12%, 15%\n\n%age\nmutant\nkill rate\non (believed;\nlikely actual)\n\n33%\n31%\n27%\n27%\n31%\n28%\n35%\n44%\n32%\n\n15\n196\n122\n9\n45\n35\n14\n135\n571\n\nrate on\n\nWhile code completions are deployed directly into the IDE in\nreal time (online), ACH is an offline technology that proposes code\nupdates in exactly the same way that engineers would propose\nthem: through Continuous Integration (CI) and standard code re-\nview. This has the advantage that the decision to accept machine-\ngenerated code has an audit trail, which is missing for LLM-based\ncode completions. By contrast, LLM-based code completion sugges-\ntions accepted by engineers are attributed to the engineer by the\nCI audit trail; details of the LLM’s role in constructing production\ncode through completion suggestions is thus unavailable.\n\nThere is already a considerable body of literature on LLMSE [22].\nThe subset of this literature devoted specifically to software testing\nis too large to survey in detail here. Instead, we refer readers to\nrecent surveys [22, 61]. In this section, we focus specifically on the\napplication of LLMs to mutation testing.\n\nTraditionally [35], mutation testing techniques have used sets of\npre-defined rule-based operators to construct mutants. However, it\nhas been widely observed [36, 51] that such pre-defined rule-based\nmutation operators are ill-suited to the task of generating realistic\nfaults. This has led researchers to consider ways to make more\nrealistic mutants [34], a problem to which language models have\nrecently been applied [57, 60]. ACH represents an extension of this\n\ndirection that additionally seeks to generate a set of faults, relevant\nto a chosen issue of concern.\n\nMutation testing has also been used as a way to evaluate lan-\nguage models themselves [38] and to mutate compiler inputs to\ntest compilers [31]. In this work, the desirable property of mutants\nis the way that they generate near neighborhoods of programs\nthat are similar to the program that they mutate, both syntactically\nand semantically. This allows researchers to explore properties of\nlanguage models when applied to code, such as their ability to\n‘understand’ the code. This ‘near neighborhood’ property has also\nbeen used in combination with language models [14] to generate\nmutants to help improve other software engineering technologies,\nsuch as Genetic Improvement [47].\n\nThe equivalent mutant problem has been widely studied [35, 40],\nand we are not the first to consider using language models to tackle\nit. Tian et al. [56] recently studied the use of language models\nto detect equivalent mutants for traditional rule-based mutation\ntesting operators on MutantBench. MutantBench [59] is a set of\nmutant-original-code pairs, written in C/C++ and Java, introduced\nin 2021 by van Hijfte and Oprescu. Tian et al. found that LLM-based\nequivalence detection outperformed traditional non-LLM-based\nequivalence detection by 35% for F1 score (the harmonic mean\n\n### Page 10\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nHarman, Sengupta\n\nof precision and recall). In particular, they highlighted the high-\nperformance of fine-tuned embeddings.\n\nAll the previous work on LLMs for mutation testing has con-\ncerned benchmark problems and empirical analysis under labora-\ntory conditions. The present paper is the first to report the deploy-\nment of LLM-based mutation testing at scale in industry. Non-LLM-\nbased mutation testing has previously been evaluated in industrial\napplication settings, at Meta [12] and at Google [48–50]. However,\nin both these previous industrial deployments, there was no attempt\nto automatically generate tests to kill the mutants. The challenging\ntask of constructing tests to kill mutants was left to engineers, who\nalso therefore had to contend with the equivalent mutant problem.\nUnlike these previous industrial deployments, ACH is also the first\nto automatically generate tests to kill mutants; a problem which has\nbeen studied in the research literature [16, 25], but not previously\ndeployed at scale in industry.\n\nThe work on ACH reported here, follows a succession of auto-\nmated analysis and testing work deployed at Meta. Our initial focus\nwas end-to-end test generation tools such as Sapienz [5, 41], and\nstatic analysis tools such as Infer [15], followed by simulation-based\ntest generation for testing interacting user communities [1, 2, 58].\nWhile much of this previous work has been concerned with system\nlevel testing, we have also worked on unit level test generation\ntechniques using observations [8] and language models [3]. How-\never, this previous unit test generation work was based on the sole\nobjective of increasing coverage, which meant it could not target\nspecific classes of faults as ACH does.\n\n8 OPEN PROBLEMS\nWe outline directions for future work, hoping to stimulate the\nresearch community with new open research questions.\nMutant Equivalence: we found that the simulated privacy\nfaults generated by language models tend to be bimodal, with the\nconsequence that equivalent mutants are surprisingly easy to detect\n(See Section 5). It is possible that these results are merely specific\nto Kotlin, to Android, to Meta, or to simulated privacy faults.\n\nHowever, there is nothing in our approach that suggests that\nthe results would fail to generalize. If they were to generalize, this\nwould be an important finding for the mutation testing research\nagenda, given the significant impediment to deployment hitherto\nposed by mutant equivalence.\nMutant Relevance: We have been able to generate specific mutants\nthat capture similar faulty behaviors as those previously witnessed.\nHowever, in some cases, anecdotally, looking at the faults generated,\nit seemed that the mutant was related to the general class of fault\nsimulated, but was not an example of the specific instance. One\ndifficulty here is that we have no way to consistently and reliably\nmeasure problem similarity or relevance.\n\nMore research is needed to define what it means for one fault to\nbe similar to another. We also need approaches that use such similar-\nity metrics to guide agenetic LLM workflows, e.g., with fine-tuning,\nprompt engineering, re-prompting, and/or Chain-of-Thought, to\nensure that the mutants generated are relevant to the original fault.\nDetecting existing faults: our approach hardens against future\nregressions. This means that the current version of the system under\nthe test is used as the regression oracle [6]: a test is deemed to pass\n\nafter some change if the test behaves the same before and after the\nchange. This allows us to protect against future regressions, but it\ncannot detect existing faults residing in the code base. To do this\nrequires us to tackle the well known Oracle problem [11].\n\nIt would be very exciting if an approach can be found to infer\noracles for Targeted LLM-based Mutation-Guided Test Generation.\nSuch an advance would be highly impactful because it would allow\nus to search for existing classes of faults. Although there has been\nmuch previous work on oracle inference [30, 44, 58, 62], we need\nhigher precision to avoid wasting engineers’ time on false positives.\nGenerating faults/tests for specific issues of concern, as ACH does,\nmay help oracle generation.\n\n9 CONCLUSIONS\nMutation testing has been the subject of research for five decades\n[20, 29, 35, 46], so has clearly retained intellectual appeal and re-\nsearchers’ curiosity. Despite this, it has proved challenging to deploy\nmutation testing in industry [4, 6, 12].\n\nTraditionally, mutants have been constructed using simple rule-\nbased approaches in order to assess test suites, largely written by\nhumans. However, software engineers need automatically gener-\nated tests, that are relevant to a specific pressing concern; their\ntime would be better spent articulating these pressing concerns,\nrather than trying to construct test cases that should, instead, be\ngenerated by a machine. Fortunately, two crucial recent advances,\ndrawn together in ACH, make this possible: Automated test gen-\neration to kill generated mutants, coupled with language models’\nability to generate highly-relevant mutants. In particular, we found\nthat LLMs help us to overcome existing barriers [12] to deployment\nof mutation testing. Specifically, they\n\n(1) Allow us to generate highly realistic faults, using their ability\nto convert text (the concern or issue) into code (simulated\nfaults from which we generate tests);\n\n(2) Provide an additional agent to weed out equivalent mutants,\nwhich is especially powerful when combined with simple\nstatic analyses as a pre-processing step;\n\n(3) Provide a way to automatically generate unit tests to kill the\n\nmutants.\n\nThis paper presented results from deployment of ACH at Meta.\nNeither LLM-based test generation, nor LLM-based mutant genera-\ntion is new, but this paper is the first to report on their combined\ndeployment on large scale industrial systems. We believe this form\nof automated test generation is highly aligned with modern soft-\nware development and deployment. It supports software engineers\nwho must contend with many competing and conflicting concerns,\noften expressed in natural language in vague, incomplete and even\ncontradictory ways. Our results also suggest Mutation-as-RAG will\nprove impactful in optimizing for structural coverage criteria.\n\nACKNOWLEDGMENTS\nWe would like to thank the Meta’s Llama team and leadership of the\nFundamental Artificial Intelligence Research (FAIR), Developer In-\nfrastructure (DevInfra) and Product Compliance and Privacy teams\nfor supporting this work and the Meta Software Engineers who so\nfreely and kindly gave of their time, experience, and expertise in\nreviewing the tests automatically generated by ACH.\n\n### Page 11\n\nMutation-Guided LLM-based Test Generation at Meta\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nREFERENCES\n[1] John Ahlgren, Maria Eugenia Berezin, Kinga Bojarczuk, Elena Dulskyte, Inna\nDvortsova, Johann George, Natalija Gucevska, Mark Harman, Maria Lomeli, Erik\nMeijer, Silvia Sapora, and Justin Spahr-Summers. 2021. Testing Web Enabled\nSimulation at Scale Using Metamorphic Testing. In International Conference on\nSoftware Engineering (ICSE) Software Engineering in Practice (SEIP) track. Virtual.\n[2] John Ahlgren, Kinga Bojarczuk, Sophia Drossopoulou, Inna Dvortsova, Johann\nGeorge, Natalija Gucevska, Mark Harman, Maria Lomeli, Simon Lucas, Erik\nMeijer, Steve Omohundro, Rubmary Rojas, Silvia Sapora, Jie M. Zhang, and Norm\nZhou. 2021. Facebook’s Cyber–Cyber and Cyber–Physical Digital Twins (keynote\npaper). In 25th International Conference on Evaluation and Assessment in Software\nEngineering (EASE 2021). Virtual.\n\n[3] Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Mark Harman, Alexan-\ndru Marginean, Shubho Sengupta, and Eddy Wang. 2024. Automated unit test\nimprovement using Large Language Models at Meta. In ACM International Con-\nference on the Foundations of Software Engineering (FSE 2024) (Porto de Galinhas,\nBrazil, Brazil).\n\n[4] Nadia Alshahwan, Andrea Ciancone, Mark Harman, Yue Jia, Ke Mao, Alexandru\nMarginean, Alexander Mols, Hila Peleg, Federica Sarro, and Ilya Zorin. 2019.\nSome challenges for software testing research (keynote paper). In Proceedings of\nthe 28th ACM SIGSOFT International Symposium on Software Testing and Analysis\n(ISSTA 2019), Beijing, China, July 15-19, 2019, Dongmei Zhang and Anders Møller\n(Eds.). ACM, 1–3.\n\n[5] Nadia Alshahwan, Xinbo Gao, Mark Harman, Yue Jia, Ke Mao, Alexander Mols,\nTaijin Tei, and Ilya Zorin. 2018. Deploying Search Based Software Engineering\nwith Sapienz at Facebook (keynote paper). In 10𝑡ℎ International Symposium\non Search Based Software Engineering (SSBSE 2018). Montpellier, France, 3–45.\nSpringer LNCS 11036.\n\n[6] Nadia Alshahwan, Mark Harman, and Alexandru Marginean. 2023. Software\nTesting Research Challenges: An Industrial Perspective (keynote paper). In 2023\nIEEE Conference on Software Testing, Verification and Validation (ICST 2023). IEEE,\n1–10.\n\n[7] Nadia Alshahwan, Mark Harman, Alexandru Marginean, Shubho Sengupta, and\nEddy Wang. 2024. Assured LLM-Based Software Engineering (keynote paper).\nIn 2𝑛𝑑 . ICSE workshop on Interoperability and Robustness of Neural Software\nEngineering (InteNSE) (Lisbon, Portugal).\n\n[8] Nadia Alshahwan, Mark Harman, Alexandru Marginean, and Eddy Wang. 2024.\nObservation-based unit test generation at Meta. In Foundations of Software Engi-\nneering (FSE 2024).\n\n[9] Kelly Androutsopoulos, David Clark, Haitao Dan, Mark Harman, and Robert\nHierons. 2014. An Analysis of the Relationship between Conditional Entropy and\nFailed Error Propagation in Software Testing. In 36𝑡ℎ International Conference\non Software Engineering (ICSE 2014). Hyderabad, India, 573–583.\n\n[10] Earl T. Barr, Yuriy Brun, Premkumar Devanbu, Mark Harman, and Federica\nSarro. 2014. The Plastic Surgery Hypothesis. In 22𝑛𝑑 ACM SIGSOFT International\nSymposium on the Foundations of Software Engineering (FSE 2014). Hong Kong,\nChina, 306–317.\n\n[11] Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo.\n2015. The Oracle Problem in Software Testing: A Survey. IEEE Transactions on\nSoftware Engineering 41, 5 (May 2015), 507–525.\n\n[12] Moritz Beller, Chu-Pan Wong, Johannes Bader, Andrew Scott, Mateusz Machalica,\nSatish Chandra, and Erik Meijer. 2021. What it would take to use mutation testing\nin industry—a study at Facebook. In 2021 IEEE/ACM 43rd International Conference\non Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 268–\n277.\n\n[13] Adam Brown, Sarah D’Angelo, Ambar Murillo, Ciera Jaspan, and Collin Green.\n2024. Identifying the Factors that Influence Trust in AI Code Completion. In 1st\nACM International Conference on AI-powered Software (AIware 2024) (Porto de\nGalinhas, Brazil).\n\n[14] Alexander Brownlee, James Callan, Karine Even-Mendoza, Alina Geiger, Carol\nHanna, Justyna Petke, Federica Sarro, and Dominik Sobania. 2023. Enhancing\ngenetic improvement mutations using large language models. In International\nSymposium on Search Based Software Engineering. Springer, 153–159.\n\n[15] C. Calcagno, D. Distefano, J. Dubreil, D. Gabi, P. Hooimeijer, M. Luca, P. W.\nO’Hearn, I. Papakonstantinou, J. Purbrick, and D. Rodriguez. 2015. Moving\nFast with Software Verification. In NASA Formal Methods - 7th International\nSymposium. 3–11.\n\n[16] Francisco Carlos, Mike Papadakis, Vinícius Durelli, and Eduardo Márcio Dela-\nmaro. 2014. Test data generation techniques for mutation testing: A systematic\nmapping. In Workshop on Experimental Software Engineering (ESELAW’14).\n[17] Thierry Titcheu Chekam, Mike Papadakis, Yves Le Traon, and Mark Harman.\n2017. An empirical study on mutation, statement and branch coverage fault\nrevelation that avoids the unreliable clean program assumption. In Proceedings\nof the 39th International Conference on Software Engineering, ICSE 2017, Buenos\nAires, Argentina, May 20-28, 2017. 597–608.\n\n[18] Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang Deng, and Jianwei\nYin. 2024. Chatunitest: A framework for LLM-based test generation. In Compan-\nion Proceedings of the 32nd ACM International Conference on the Foundations of\n\nSoftware Engineering. 572–576.\n\n[19] Henry Coles, Thomas Laurent, Christopher Henard, Mike Papadakis, and An-\nthony Ventresque. 2016. Pit: a practical mutation testing tool for Java. In Pro-\nceedings of the 25th international symposium on software testing and analysis.\n449–452.\n\n[20] Richard A. DeMillo, Richard J. Lipton, and Frederick G. Sayward. 1978. Hints on\ntest data selection: Help for the practical programmer. IEEE Computer 11 (1978),\n31–41.\n\n[21] José Javier Dolado, Mark Harman, Mari Carmen Otero, and Lin Hu. 2003. An\nempirical investigation of the influence of a type of side effects on program\ncomprehension. IEEE Transactions on Software Engineering 29, 7 (2003), 665–670.\n[22] Angela Fan, Beliz Gokkaya, Mitya Lyubarskiy, Mark Harman, Shubho Sengupta,\nShin Yoo, and Jie Zhang. 2023. Large Language Models for Software Engineering:\nSurvey and Open Problems. In ICSE Future of Software Engineering (FoSE 2023).\n[23] Mark Gabel and Zhendong Su. 2010. A Study of the Uniqueness of Source Code.\n\nIn FSE. 147–156.\n\n[24] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu,\nWei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. A Survey on\nLLM-as-a-Judge. arXiv preprint arXiv:2411.15594 (2024).\n\n[25] Mark Harman, Yue Jia, and William B. Langdon. 2011. Strong Higher Order\nMutation-Based Test Data Generation. In 8𝑡ℎ European Software Engineering\nConference and the ACM SIGSOFT Symposium on the Foundations of Software\nEngineering (ESEC/FSE ’11) (Szeged, Hungary). ACM, New York, NY, USA, 212–\n222.\n\n[26] Mark Harman and Peter O’Hearn. 2018. From Start-ups to Scale-ups: Opportu-\nnities and Open Problems for Static and Dynamic Program Analysis (keynote\npaper). In 18𝑡ℎ IEEE International Working Conference on Source Code Analysis\nand Manipulation (SCAM 2018). Madrid, Spain, 1–23.\n\n[27] Mark Harman, Xiangjuan Yao, and Yue Jia. 2014. A Study of Equivalent and\nStubborn Mutation Operators Using Human Analysis of Equivalence. In 36𝑡ℎ\nInternational Conference on Software Engineering (ICSE 2014). Hyderabad, India,\n919–930.\n\n[28] Abram Hindle, Earl Barr, Zhendong Su, Prem Devanbu, and Mark Gabel. 2012. On\nthe Naturalness of Software. In International Conference on Software Engineering\n(ICSE 2012). Zurich, Switzerland.\n\n[29] William E. Howden. 1985. Theory and Practice of Functional Testing.\n\nIEEE\n\nSoftware 2, 5 (Sept. 1985), 6–17.\n\n[30] Ali Reza Ibrahimzada, Yigit Varli, Dilara Tekinoglu, and Reyhaneh Jabbarvand.\n2022. Perfect is the enemy of test oracle. In Proceedings of the 30th ACM Joint\nEuropean Software Engineering Conference and Symposium on the Foundations of\nSoftware Engineering. 70–81.\n\n[31] Davide Italiano and Chris Cummins. 2025. Finding missed code size optimizations\n\nin compilers using LLMs. In Compiler Construction. To appear.\n\n[32] Andrew T Jebb, Vincent Ng, and Louis Tay. 2021. A review of key Likert scale\n\ndevelopment advances: 1995–2019. Frontiers in psychology 12, 637547 (2021).\n\n[33] Yue Jia and Mark Harman. 2008. Milu: A Customizable, Runtime-Optimized\nHigher Order Mutation Testing Tool for the Full C Language. In 3𝑟𝑑 Testing\nAcademia and Industry Conference - Practice and Research Techniques (TAIC\nPART’08). Windsor, UK, 94–98.\n\n[34] Yue Jia and Mark Harman. 2009. Higher Order Mutation Testing. Journal of\n\nInformation and Software Technology 51, 10 (2009), 1379–1393.\n\n[35] Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development of\nMutation Testing. IEEE Transactions on Software Engineering 37, 5 (September–\nOctober 2011), 649 – 678.\n\n[36] Matthieu Jimenez, Thierry Titcheu Chekam, Maxime Cordy, Mike Papadakis,\nMarinos Kintis, Yves Le Traon, and Mark Harman. 2018. Are mutants really\nnatural?: a study on how \"naturalness\" helps mutant selection. In Proceedings of\nthe 12th ACM/IEEE International Symposium on Empirical Software Engineering\nand Measurement, ESEM 2018, Oulu, Finland, October 11-12, 2018, Markku Oivo,\nDaniel Méndez Fernández, and Audris Mockus (Eds.). ACM, 3:1–3:10.\n\n[37] René Just. 2014. The Major mutation framework: Efficient and scalable mutation\nanalysis for Java. In Proceedings of the 2014 international symposium on software\ntesting and analysis. 433–436.\n\n[38] Ziyu Li and Donghwan Shin. 2024. Mutation-based consistency testing for\nevaluating the code understanding capability of LLMs. In Proceedings of the\nIEEE/ACM 3rd International Conference on AI Engineering-Software Engineering\nfor AI. 150–159.\n\n[39] Kaibo Liu, Yiyang Liu, Zhenpeng Chen, Jie M Zhang, Yudong Han, Yun Ma, Ge\nLi, and Gang Huang. 2024. LLM-Powered Test Case Generation for Detecting\nTricky Bugs. arXiv preprint arXiv:2404.10304 (2024).\n\n[40] Lech Madeyski, Wojciech Orzeszyna, Richard Torkar, and Mariusz Jozala. 2013.\nOvercoming the equivalent mutant problem: A systematic literature review and a\ncomparative experiment of second order mutation. IEEE Transactions on Software\nEngineering 40, 1 (2013), 23–42.\n\n[41] Ke Mao, Mark Harman, and Yue Jia. 2016. Sapienz: Multi-objective Automated\nTesting for Android Applications. In International Symposium on Software Testing\nand Analysis (ISSTA 2016). 94–105.\n\n### Page 12\n\nFSE Companion ’25, 23 – 27, 2025, Trondheim, Norway\n\nHarman, Sengupta\n\n[42] Meta. 2024. Introducing Llama 3.1: Our most capable models to date. https:\n\n//ai.meta.com/blog/meta-llama-3-1/\n\n[43] Milos Ojdanic, Mike Papadakis, and Mark Harman. 2023. Keeping mutation test\nsuites consistent and relevant with long-standing mutants. In Proceedings of the\n31st ACM Joint European Software Engineering Conference and Symposium on the\nFoundations of Software Engineering. 2067–2071.\n\n[44] Rafael AP Oliveira, Upulee Kanewala, and Paulo A Nardi. 2014. Automated test\noracles: State of the art, taxonomies, and trends. Advances in computers 95 (2014),\n113–199.\n\n[45] Mike Papadakis, Yue Jia, Mark Harman, and Yves Le Traon. 2015. Trivial Com-\npiler Equivalence: A Large Scale Empirical Study of a Simple, Fast and Effective\nEquivalent Mutant Detection Technique. In 37𝑡ℎ International Conference on\nSoftware Engineering (ICSE 2015). Florence, Italy, 936–946.\n\n[46] Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and Mark\nHarman. 2019. Mutation Testing Advances: An Analysis and Survey. Advances\nin Computers 112 (2019), 275–378.\n\n[47] Justyna Petke, Saemundur O. Haraldsson, Mark Harman, William B. Langdon,\nDavid R. White, and John R. Woodward. 2018. Genetic Improvement of Software:\na Comprehensive Survey. IEEE Transactions on Evolutionary Computation 22, 3\n(June 2018), 415–432.\n\n[48] Goran Petrović and Marko Ivanković. 2018. State of mutation testing at Google. In\nProceedings of the 40th international conference on software engineering: Software\nengineering in practice. 163–171.\n\n[49] Goran Petrović, Marko Ivanković, Gordon Fraser, and René Just. 2021. Practical\nmutation testing at scale: A view from Google. IEEE Transactions on Software\nEngineering 48, 10 (2021), 3900–3912.\n\n[50] Goran Petrovic, Marko Ivankovic, Bob Kurtz, Paul Ammann, and René Just. 2018.\nAn industrial application of mutation testing: Lessons, challenges, and research\ndirections. In 2018 IEEE International Conference on Software Testing, Verification\nand Validation Workshops (ICSTW). IEEE, 47–53.\n\n[51] Cedric Richter and Heike Wehrheim. 2022. Learning realistic mutations: Bug\ncreation for neural bug detectors. In 2022 IEEE Conference on Software Testing,\nVerification and Validation (ICST). IEEE, 162–173.\n\n[52] Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma, Mu-\nrali Krishna Ramanathan, and Baishakhi Ray. 2024. Code-aware prompting: A\nstudy of coverage-guided test generation in regression setting using LLM. Pro-\nceedings of the ACM on Conference on Foundations of Software Engineering 1, FSE\n(2024), 951–971.\n\n[53] Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. An empirical\nevaluation of using large language models for automated unit test generation.\nIEEE Transactions on Software Engineering (2023).\n\n[54] David Schuler and Andreas Zeller. 2009. Javalanche: efficient mutation testing for\nJava. In 7𝑡ℎ joint meeting of the European Software Engineering Conference and the\nACM SIGSOFT International Symposium on Foundations of Software Engineering\n(ESEC/FSE 2009). 297–298.\n\n[55] Richard Speed. 2023. GitHub: 30% of Copilot coding suggestions are ac-\ncepted. https://www.itpro.com/technology/artificial-intelligence/github-30-of-\ncopilot-coding-suggestions-are-accepted?utm_source=chatgpt.com\n\n[56] Zhao Tian, Honglin Shu, Dong Wang, Xuejie Cao, Yasutaka Kamei, and Junjie\nChen. 2024. Large Language Models for Equivalent Mutant Detection: How Far\nAre We?. In Proceedings of the 33rd ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis. 1733–1745.\n\n[57] Frank Tip, Jonathan Bell, and Max Schäfer. 2024. LLMorpheus: Mutation Testing\n\nusing Large Language Models. arXiv preprint arXiv:2404.09952 (2024).\n\n[58] Shreshth Tuli, Kinga Bojarczuk, Natalija Gucevska, Mark Harman, Xiao-Yu Wang,\nand Graham Wright. 2023. Simulation-Driven Automated End-to-End Test and\nOracle Inference. In 45th IEEE/ACM International Conference on Software Engi-\nneering: Software Engineering in Practice, SEIP@ICSE 2023, Melbourne, Australia,\nMay 14-20, 2023. IEEE, 122–133.\n\n[59] Lars van Hijfte and Ana Oprescu. 2021. Mutantbench: an equivalent mutant\nproblem comparison framework. In 2021 IEEE International Conference on Software\nTesting, Verification and Validation Workshops (ICSTW). IEEE, 7–12.\n\n[60] Bo Wang, Mingda Chen, Youfang Lin, Mike Papadakis, and Jie M Zhang. 2024.\nAn Exploratory Study on Using Large Language Models xfor Mutation Testing.\narXiv preprint arXiv:2406.09843 (2024).\n\n[61] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing\nWang. 2023. Software Testing with Large Language Model: Survey, Landscape,\nand Vision. arXiv:2307.07221.\n\n[62] Jie Zhang, Junjie Chen, Dan Hao, Yingfei Xiong, Bing Xie, Lu Zhang, and Hong\nMei. 2014. Search-based inference of polynomial metamorphic relations. In\nACM/IEEE International Conference on Automated Software Engineering (ASE’14),\nIvica Crnkovic, Marsha Chechik, and Paul Gruenbacher (Eds.). Vasteras, Sweden,\n701–712.\n\n[63] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judg-\ning LLM-as-a-judge with MT-bench and chatbot arena. Advances in Neural\nInformation Processing Systems (NeurIPS 2023) 36 (2023), 46595–46623.",
    "content_summary": "### Page 1\n\n5\n2\n0\n2\n\nn\na\nJ\n\n2\n2\n\n]\nE\nS\n.\ns\nc\n[\n\n1\nv\n2\n6\n8\n2\n1\n.\n1\n0\n5\n2\n:\nv\ni\nX\nr\na\n\nMutation-Guided LLM-based Test Generation at Meta\nChristopher Foster\nAbhishek Gulati\nProduct Compliance and Privacy\nProduct Compliance and Privacy\nteam, Meta Platfor...",
    "content_length": 75004,
    "created_at": "2025-05-06T21:28:22.720536",
    "updated_at": "2025-05-06T21:28:22.720536",
    "file_path": "unknown_source"
  },
  "doc-3c1b7700a655e44dd23947305245e24a": {
    "status": "pending",
    "content": "ACH is automatic complaince hardener",
    "content_summary": "ACH is automatic complaince hardener",
    "content_length": 36,
    "created_at": "2025-05-06T21:29:02.565332",
    "updated_at": "2025-05-06T21:29:02.565332",
    "file_path": "unknown_source"
  }
}